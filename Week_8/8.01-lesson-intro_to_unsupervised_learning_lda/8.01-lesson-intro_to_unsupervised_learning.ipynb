{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyLDAvis\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/3a/af82e070a8a96e13217c8f362f9a73e82d61ac8fff3a2561946a97f96266/pyLDAvis-2.1.2.tar.gz (1.6MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.6MB 2.1MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.23.0 in /Users/admin/anaconda3/envs/dsi/lib/python3.6/site-packages (from pyLDAvis) (0.31.0)\n",
      "Requirement already satisfied: numpy>=1.9.2 in /Users/admin/anaconda3/envs/dsi/lib/python3.6/site-packages (from pyLDAvis) (1.14.3)\n",
      "Requirement already satisfied: scipy>=0.18.0 in /Users/admin/anaconda3/envs/dsi/lib/python3.6/site-packages (from pyLDAvis) (1.1.0)\n",
      "Requirement already satisfied: pandas>=0.17.0 in /Users/admin/anaconda3/envs/dsi/lib/python3.6/site-packages (from pyLDAvis) (0.23.0)\n",
      "Requirement already satisfied: joblib>=0.8.4 in /Users/admin/anaconda3/envs/dsi/lib/python3.6/site-packages (from pyLDAvis) (0.11)\n",
      "Requirement already satisfied: jinja2>=2.7.2 in /Users/admin/anaconda3/envs/dsi/lib/python3.6/site-packages (from pyLDAvis) (2.10)\n",
      "Collecting numexpr (from pyLDAvis)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e2/ef/f37e4f11eadc37af2aaf85cd8b13ca27724f67fc28c185bac6eaa8bddb03/numexpr-2.6.5-cp36-cp36m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (166kB)\n",
      "\u001b[K    100% |████████████████████████████████| 174kB 2.0MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting pytest (from pyLDAvis)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/75/e79b66c9fe6166a90004bb8fb02bab06213c3348e93f3be41d7eaf625554/pytest-3.6.1-py2.py3-none-any.whl (194kB)\n",
      "\u001b[K    100% |████████████████████████████████| 194kB 1.8MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting future (from pyLDAvis)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/2b/8d082ddfed935f3608cc61140df6dcbf0edea1bc3ab52fb6c29ae3e81e85/future-0.16.0.tar.gz (824kB)\n",
      "\u001b[K    100% |████████████████████████████████| 829kB 1.9MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting funcy (from pyLDAvis)\n",
      "  Downloading https://files.pythonhosted.org/packages/46/c6/8a9e1489759fb97e175ccd1405abab1f82d6bd9284893ef74cd345134ab3/funcy-1.10.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /Users/admin/anaconda3/envs/dsi/lib/python3.6/site-packages (from pandas>=0.17.0->pyLDAvis) (2.7.2)\n",
      "Requirement already satisfied: pytz>=2011k in /Users/admin/anaconda3/envs/dsi/lib/python3.6/site-packages (from pandas>=0.17.0->pyLDAvis) (2018.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/admin/anaconda3/envs/dsi/lib/python3.6/site-packages (from jinja2>=2.7.2->pyLDAvis) (1.0)\n",
      "Collecting more-itertools>=4.0.0 (from pytest->pyLDAvis)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/40/90c3b0393e12b9827381004224de8814686e3d7182f9d4182477f600826d/more_itertools-4.2.0-py3-none-any.whl (45kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 2.0MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /Users/admin/anaconda3/envs/dsi/lib/python3.6/site-packages (from pytest->pyLDAvis) (39.0.1)\n",
      "Collecting attrs>=17.4.0 (from pytest->pyLDAvis)\n",
      "  Downloading https://files.pythonhosted.org/packages/41/59/cedf87e91ed541be7957c501a92102f9cc6363c623a7666d69d51c78ac5b/attrs-18.1.0-py2.py3-none-any.whl\n",
      "Collecting atomicwrites>=1.0 (from pytest->pyLDAvis)\n",
      "  Downloading https://files.pythonhosted.org/packages/0a/e8/cd6375e7a59664eeea9e1c77a766eeac0fc3083bb958c2b41ec46b95f29c/atomicwrites-1.1.5-py2.py3-none-any.whl\n",
      "Collecting py>=1.5.0 (from pytest->pyLDAvis)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/a5/f77982214dd4c8fd104b066f249adea2c49e25e8703d284382eb5e9ab35a/py-1.5.3-py2.py3-none-any.whl (84kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 2.3MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /Users/admin/anaconda3/envs/dsi/lib/python3.6/site-packages (from pytest->pyLDAvis) (1.11.0)\n",
      "Collecting pluggy<0.7,>=0.5 (from pytest->pyLDAvis)\n",
      "  Downloading https://files.pythonhosted.org/packages/ba/65/ded3bc40bbf8d887f262f150fbe1ae6637765b5c9534bd55690ed2c0b0f7/pluggy-0.6.0-py3-none-any.whl\n",
      "Building wheels for collected packages: pyLDAvis, future\n",
      "  Running setup.py bdist_wheel for pyLDAvis ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/admin/Library/Caches/pip/wheels/98/71/24/513a99e58bb6b8465bae4d2d5e9dba8f0bef8179e3051ac414\n",
      "  Running setup.py bdist_wheel for future ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/admin/Library/Caches/pip/wheels/bf/c9/a3/c538d90ef17cf7823fa51fc701a7a7a910a80f6a405bf15b1a\n",
      "Successfully built pyLDAvis future\n",
      "Installing collected packages: numexpr, more-itertools, attrs, atomicwrites, py, pluggy, pytest, future, funcy, pyLDAvis\n",
      "Successfully installed atomicwrites-1.1.5 attrs-18.1.0 funcy-1.10.2 future-0.16.0 more-itertools-4.2.0 numexpr-2.6.5 pluggy-0.6.0 py-1.5.3 pyLDAvis-2.1.2 pytest-3.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# NLTK\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Food and Animals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_1 = 'I like to eat broccoli and bananas.'\n",
    "doc_2 = 'I ate a banana and spinach smoothie for breakfast.'\n",
    "doc_3 = 'Chinchillas and kittens are cute.'\n",
    "doc_4 = 'My sister adopted a kitten yesterday.'\n",
    "doc_5 = 'Look at this cute hamster munching on a piece of broccoli.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Preprocess our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(text):\n",
    "    '''\n",
    "    Takes in a string of text, then performs the following:\n",
    "        1. Tokenizes and removes punctuation\n",
    "        2. Removes stopwords\n",
    "        3. Stems\n",
    "        4. Returns a list of the cleaned text\n",
    "    '''\n",
    "    if pd.isnull(text):\n",
    "        return []\n",
    "    # tokenizing and removing punctuation\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    text_processed=tokenizer.tokenize(text)\n",
    "    \n",
    "    # removing any stopwords\n",
    "    text_processed = [word.lower() for word in text_processed if word.lower() not in stopwords.words('english')]\n",
    "    \n",
    "    # stemming\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    \n",
    "    text_processed = [porter_stemmer.stem(word) for word in text_processed]\n",
    "    \n",
    "    try:\n",
    "        text_processed.remove('b')\n",
    "    except: \n",
    "        pass\n",
    "\n",
    "    return text_processed ## <-- we're keeping our words distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['like', 'eat', 'broccoli', 'banana']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_process(doc_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=(text_process(doc_1),\n",
    "      text_process(doc_2),\n",
    "      text_process(doc_3),\n",
    "      text_process(doc_4),\n",
    "      text_process(doc_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['like', 'eat', 'broccoli', 'banana'],\n",
       " ['ate', 'banana', 'spinach', 'smoothi', 'breakfast'],\n",
       " ['chinchilla', 'kitten', 'cute'],\n",
       " ['sister', 'adopt', 'kitten', 'yesterday'],\n",
       " ['look', 'cute', 'hamster', 'munch', 'piec', 'broccoli'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Fit LDA Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook() # for the visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary=corpora.Dictionary(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.corpora.dictionary.Dictionary at 0x1a26e7a160>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[dictionary.doc2bow(text) for text in texts] # turn each documents into a bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate\n",
    "ldamodel=models.ldamodel.LdaModel(corpus, # pass in our corpus\n",
    "                                 id2word= dictionary, ## match documents to spot in dict\n",
    "                                 num_topics=3,# Hyperparameter num of topics\n",
    "                                 passes=5,\n",
    "                                 minimum_probability=0.01) #how many times do we run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1)]\n",
      "[(0, 1), (4, 1), (5, 1), (6, 1), (7, 1)]\n",
      "[(8, 1), (9, 1), (10, 1)]\n",
      "[(10, 1), (11, 1), (12, 1), (13, 1)]\n",
      "[(1, 1), (9, 1), (14, 1), (15, 1), (16, 1), (17, 1)]\n"
     ]
    }
   ],
   "source": [
    "for text in texts:\n",
    "    print(dictionary.doc2bow(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Visualize LDA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/anaconda3/envs/dsi/lib/python3.6/site-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el102911123309542164841415182\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el102911123309542164841415182_data = {\"mdsDat\": {\"x\": [0.07885098459106271, -0.0710766672694379, -0.0077743173216248485], \"y\": [-0.03190693841184935, -0.043662647205271146, 0.07556958561712047], \"topics\": [1, 2, 3], \"cluster\": [1, 1, 1], \"Freq\": [53.96989440917969, 25.1719913482666, 20.85811424255371]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\"], \"Freq\": [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.4609434604644775, 1.4591233730316162, 0.8386264443397522, 0.8385100960731506, 0.8365816473960876, 0.8298346400260925, 0.8297410607337952, 0.8285068869590759, 0.8279404640197754, 0.8152696490287781, 0.8281222581863403, 0.21191926300525665, 0.21181057393550873, 0.21139951050281525, 0.21143217384815216, 0.21126410365104675, 0.2112162858247757, 0.21113507449626923, 0.6621291637420654, 0.6619759202003479, 0.6617083549499512, 0.6615185737609863, 0.6717668771743774, 0.16693323850631714, 0.16678741574287415, 0.16671961545944214, 0.17567755281925201, 0.17511267960071564, 0.17430216073989868, 0.1741945594549179, 0.16779522597789764, 0.16721320152282715, 0.16708242893218994, 0.16858159005641937, 0.1745305359363556, 0.17380912601947784, 0.6064878106117249, 0.6060502529144287, 0.6059950590133667, 0.6246320605278015, 0.15300922095775604, 0.15295669436454773, 0.1527460515499115, 0.15266427397727966, 0.15398406982421875, 0.15310977399349213, 0.15300120413303375, 0.15299227833747864, 0.15296123921871185, 0.15316054224967957, 0.15312470495700836, 0.15420910716056824, 0.15417712926864624, 0.1535237729549408], \"Term\": [\"sister\", \"adopt\", \"yesterday\", \"spinach\", \"ate\", \"smoothi\", \"breakfast\", \"kitten\", \"banana\", \"broccoli\", \"cute\", \"chinchilla\", \"like\", \"eat\", \"look\", \"hamster\", \"piec\", \"munch\", \"broccoli\", \"cute\", \"like\", \"eat\", \"chinchilla\", \"look\", \"hamster\", \"piec\", \"munch\", \"kitten\", \"banana\", \"adopt\", \"yesterday\", \"sister\", \"breakfast\", \"smoothi\", \"ate\", \"spinach\", \"spinach\", \"ate\", \"smoothi\", \"breakfast\", \"banana\", \"yesterday\", \"adopt\", \"sister\", \"munch\", \"piec\", \"hamster\", \"look\", \"chinchilla\", \"eat\", \"like\", \"kitten\", \"cute\", \"broccoli\", \"sister\", \"adopt\", \"yesterday\", \"kitten\", \"breakfast\", \"smoothi\", \"ate\", \"spinach\", \"chinchilla\", \"piec\", \"munch\", \"look\", \"hamster\", \"like\", \"eat\", \"banana\", \"cute\", \"broccoli\"], \"Total\": [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.7882764339447021, 1.7878310680389404, 1.1588695049285889, 1.1588479280471802, 1.1583609580993652, 1.157021403312683, 1.157004475593567, 1.1567293405532837, 1.1566191911697388, 1.6084833145141602, 1.6540982723236084, 0.9847569465637207, 0.9847388863563538, 0.984606921672821, 1.0259599685668945, 1.0259292125701904, 1.0259382724761963, 1.0259284973144531, 1.0259284973144531, 1.0259382724761963, 1.0259292125701904, 1.0259599685668945, 1.6540982723236084, 0.9847388863563538, 0.9847569465637207, 0.984606921672821, 1.1566191911697388, 1.1567293405532837, 1.157004475593567, 1.157021403312683, 1.1583609580993652, 1.1588479280471802, 1.1588695049285889, 1.6084833145141602, 1.7878310680389404, 1.7882764339447021, 0.984606921672821, 0.9847569465637207, 0.9847388863563538, 1.6084833145141602, 1.0259599685668945, 1.0259292125701904, 1.0259382724761963, 1.0259284973144531, 1.1583609580993652, 1.1567293405532837, 1.1566191911697388, 1.157021403312683, 1.157004475593567, 1.1588695049285889, 1.1588479280471802, 1.6540982723236084, 1.7878310680389404, 1.7882764339447021], \"loglift\": [18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.4146000146865845, 0.41359999775886536, 0.29330000281333923, 0.29319998621940613, 0.2912999987602234, 0.28439998626708984, 0.2842999994754791, 0.28299999237060547, 0.2824000120162964, -0.06279999762773514, -0.07509999722242355, -0.9193999767303467, -0.9199000000953674, -0.9217000007629395, -0.9627000093460083, -0.9635000228881836, -0.963699996471405, -0.9641000032424927, 0.9415000081062317, 0.9412999749183655, 0.9409000277519226, 0.9405999779701233, 0.478300005197525, -0.3953000009059906, -0.3962000012397766, -0.39649999141693115, -0.5052000284194946, -0.5084999799728394, -0.5134000182151794, -0.5139999985694885, -0.5526000261306763, -0.5565000176429749, -0.5572999715805054, -0.8762000203132629, -0.9472000002861023, -0.9516000151634216, 1.082900047302246, 1.0820000171661377, 1.0819000005722046, 0.6215000152587891, -0.33550000190734863, -0.3357999920845032, -0.33719998598098755, -0.3377000093460083, -0.4505000114440918, -0.454800009727478, -0.4553999900817871, -0.45579999685287476, -0.4560000002384186, -0.4562999904155731, -0.45649999380111694, -0.8052999973297119, -0.8831999897956848, -0.8877000212669373], \"logprob\": [18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -2.0952000617980957, -2.0964999198913574, -2.6503000259399414, -2.650399923324585, -2.6526999473571777, -2.660799980163574, -2.660900115966797, -2.662400007247925, -2.663100004196167, -2.678499937057495, -2.662899971008301, -4.0258002281188965, -4.026400089263916, -4.028299808502197, -4.02810001373291, -4.028900146484375, -4.029200077056885, -4.029600143432617, -2.1238999366760254, -2.1240999698638916, -2.124500036239624, -2.124799966812134, -2.1094000339508057, -3.501800060272217, -3.5025999546051025, -3.503000020980835, -3.450700044631958, -3.453900098800659, -3.4586000442504883, -3.459199905395508, -3.4965999126434326, -3.5000998973846436, -3.5009000301361084, -3.4918999671936035, -3.4572999477386475, -3.461400032043457, -2.023699998855591, -2.024399995803833, -2.0244998931884766, -1.9941999912261963, -3.400899887084961, -3.40120005607605, -3.402600049972534, -3.40310001373291, -3.3945000171661377, -3.4001998901367188, -3.400899887084961, -3.4010000228881836, -3.40120005607605, -3.399899959564209, -3.400099992752075, -3.3931000232696533, -3.3933000564575195, -3.3975000381469727]}, \"token.table\": {\"Topic\": [3, 2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 3, 2, 2, 3], \"Freq\": [1.0154789686203003, 0.9747174978256226, 0.6045590043067932, 0.6045590043067932, 0.9746968746185303, 0.5591976642608643, 0.8632887601852417, 0.5593369603157043, 0.8629260063171387, 0.8643009066581726, 0.6217036843299866, 0.6217036843299866, 0.8629099130630493, 0.8642882704734802, 0.8645887970924377, 0.8645064830780029, 1.0156337022781372, 0.974726140499115, 0.9747267961502075, 1.0154975652694702], \"Term\": [\"adopt\", \"ate\", \"banana\", \"banana\", \"breakfast\", \"broccoli\", \"chinchilla\", \"cute\", \"eat\", \"hamster\", \"kitten\", \"kitten\", \"like\", \"look\", \"munch\", \"piec\", \"sister\", \"smoothi\", \"spinach\", \"yesterday\"]}, \"R\": 18, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [3, 1, 2]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el102911123309542164841415182\", ldavis_el102911123309542164841415182_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el102911123309542164841415182\", ldavis_el102911123309542164841415182_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el102911123309542164841415182\", ldavis_el102911123309542164841415182_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "2      0.078851 -0.031907       1        1  53.969894\n",
       "0     -0.071077 -0.043663       2        1  25.171991\n",
       "1     -0.007774  0.075570       3        1  20.858114, topic_info=     Category      Freq        Term     Total  loglift  logprob\n",
       "term                                                           \n",
       "12    Default  0.000000      sister  0.000000  18.0000  18.0000\n",
       "11    Default  0.000000       adopt  0.000000  17.0000  17.0000\n",
       "13    Default  0.000000   yesterday  0.000000  16.0000  16.0000\n",
       "7     Default  1.000000     spinach  1.000000  15.0000  15.0000\n",
       "4     Default  1.000000         ate  1.000000  14.0000  14.0000\n",
       "6     Default  1.000000     smoothi  1.000000  13.0000  13.0000\n",
       "5     Default  1.000000   breakfast  1.000000  12.0000  12.0000\n",
       "10    Default  1.000000      kitten  1.000000  11.0000  11.0000\n",
       "0     Default  1.000000      banana  1.000000  10.0000  10.0000\n",
       "1     Default  1.000000    broccoli  1.000000   9.0000   9.0000\n",
       "9     Default  1.000000        cute  1.000000   8.0000   8.0000\n",
       "8     Default  1.000000  chinchilla  1.000000   7.0000   7.0000\n",
       "3     Default  1.000000        like  1.000000   6.0000   6.0000\n",
       "2     Default  1.000000         eat  1.000000   5.0000   5.0000\n",
       "15    Default  1.000000        look  1.000000   4.0000   4.0000\n",
       "14    Default  1.000000     hamster  1.000000   3.0000   3.0000\n",
       "17    Default  1.000000        piec  1.000000   2.0000   2.0000\n",
       "16    Default  1.000000       munch  1.000000   1.0000   1.0000\n",
       "1      Topic1  1.460943    broccoli  1.788276   0.4146  -2.0952\n",
       "9      Topic1  1.459123        cute  1.787831   0.4136  -2.0965\n",
       "3      Topic1  0.838626        like  1.158870   0.2933  -2.6503\n",
       "2      Topic1  0.838510         eat  1.158848   0.2932  -2.6504\n",
       "8      Topic1  0.836582  chinchilla  1.158361   0.2913  -2.6527\n",
       "15     Topic1  0.829835        look  1.157021   0.2844  -2.6608\n",
       "14     Topic1  0.829741     hamster  1.157004   0.2843  -2.6609\n",
       "17     Topic1  0.828507        piec  1.156729   0.2830  -2.6624\n",
       "16     Topic1  0.827940       munch  1.156619   0.2824  -2.6631\n",
       "10     Topic1  0.815270      kitten  1.608483  -0.0628  -2.6785\n",
       "0      Topic1  0.828122      banana  1.654098  -0.0751  -2.6629\n",
       "11     Topic1  0.211919       adopt  0.984757  -0.9194  -4.0258\n",
       "...       ...       ...         ...       ...      ...      ...\n",
       "11     Topic2  0.166787       adopt  0.984757  -0.3962  -3.5026\n",
       "12     Topic2  0.166720      sister  0.984607  -0.3965  -3.5030\n",
       "16     Topic2  0.175678       munch  1.156619  -0.5052  -3.4507\n",
       "17     Topic2  0.175113        piec  1.156729  -0.5085  -3.4539\n",
       "14     Topic2  0.174302     hamster  1.157004  -0.5134  -3.4586\n",
       "15     Topic2  0.174195        look  1.157021  -0.5140  -3.4592\n",
       "8      Topic2  0.167795  chinchilla  1.158361  -0.5526  -3.4966\n",
       "2      Topic2  0.167213         eat  1.158848  -0.5565  -3.5001\n",
       "3      Topic2  0.167082        like  1.158870  -0.5573  -3.5009\n",
       "10     Topic2  0.168582      kitten  1.608483  -0.8762  -3.4919\n",
       "9      Topic2  0.174531        cute  1.787831  -0.9472  -3.4573\n",
       "1      Topic2  0.173809    broccoli  1.788276  -0.9516  -3.4614\n",
       "12     Topic3  0.606488      sister  0.984607   1.0829  -2.0237\n",
       "11     Topic3  0.606050       adopt  0.984757   1.0820  -2.0244\n",
       "13     Topic3  0.605995   yesterday  0.984739   1.0819  -2.0245\n",
       "10     Topic3  0.624632      kitten  1.608483   0.6215  -1.9942\n",
       "5      Topic3  0.153009   breakfast  1.025960  -0.3355  -3.4009\n",
       "6      Topic3  0.152957     smoothi  1.025929  -0.3358  -3.4012\n",
       "4      Topic3  0.152746         ate  1.025938  -0.3372  -3.4026\n",
       "7      Topic3  0.152664     spinach  1.025928  -0.3377  -3.4031\n",
       "8      Topic3  0.153984  chinchilla  1.158361  -0.4505  -3.3945\n",
       "17     Topic3  0.153110        piec  1.156729  -0.4548  -3.4002\n",
       "16     Topic3  0.153001       munch  1.156619  -0.4554  -3.4009\n",
       "15     Topic3  0.152992        look  1.157021  -0.4558  -3.4010\n",
       "14     Topic3  0.152961     hamster  1.157004  -0.4560  -3.4012\n",
       "3      Topic3  0.153161        like  1.158870  -0.4563  -3.3999\n",
       "2      Topic3  0.153125         eat  1.158848  -0.4565  -3.4001\n",
       "0      Topic3  0.154209      banana  1.654098  -0.8053  -3.3931\n",
       "9      Topic3  0.154177        cute  1.787831  -0.8832  -3.3933\n",
       "1      Topic3  0.153524    broccoli  1.788276  -0.8877  -3.3975\n",
       "\n",
       "[72 rows x 6 columns], token_table=      Topic      Freq        Term\n",
       "term                             \n",
       "11        3  1.015479       adopt\n",
       "4         2  0.974717         ate\n",
       "0         1  0.604559      banana\n",
       "0         2  0.604559      banana\n",
       "5         2  0.974697   breakfast\n",
       "1         1  0.559198    broccoli\n",
       "8         1  0.863289  chinchilla\n",
       "9         1  0.559337        cute\n",
       "2         1  0.862926         eat\n",
       "14        1  0.864301     hamster\n",
       "10        1  0.621704      kitten\n",
       "10        3  0.621704      kitten\n",
       "3         1  0.862910        like\n",
       "15        1  0.864288        look\n",
       "16        1  0.864589       munch\n",
       "17        1  0.864506        piec\n",
       "12        3  1.015634      sister\n",
       "6         2  0.974726     smoothi\n",
       "7         2  0.974727     spinach\n",
       "13        3  1.015498   yesterday, R=18, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[3, 1, 2])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Update model with new data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_6 = 'That cat is so cute! It looks good enough to eat.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Yelp Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review = pd.read_json(\"./yelp_academic_dataset_review.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
