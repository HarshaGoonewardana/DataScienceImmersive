{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Neural Networks with TensorFlow\n",
    "\n",
    "_Authors: Justin Pounders (ATL) and Riley Dalles (ATX)_\n",
    "\n",
    "---\n",
    "\n",
    "<a id=\"learning-objectives\"></a>\n",
    "### Learning Objectives\n",
    "- Describe the basic `tensorflow` workflow.\n",
    "- Create computational graphs representing basic feed-forward neural networks.\n",
    "- Train neural networks using `tensorflow`\n",
    "- Create and train neural networks for both regression and classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Graphs\n",
    "\n",
    "---\n",
    "\n",
    "Tensorflow is fundamentally a library for creating **computational graphs**.\n",
    "\n",
    "![](assets/comp_graph.png)\n",
    "\n",
    "Let's define and evaluate this \"computational graph.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are two phases to building a `tensorflow` model.**\n",
    "\n",
    "1. Graph construction\n",
    "2. Training/evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the graph\n",
    "a=tf.Variable(3,name='a')\n",
    "b=tf.Variable(4,name='b')\n",
    "c=a+b\n",
    "d=b+1\n",
    "e=c*d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'mul:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the graph\n",
    "init=tf.global_variables_initializer() # load the tensorflow \n",
    "# running tensorflow session as sess:\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    result=sess.run(e)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Neural Network for Regression\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by building a neural net for _regression_.  These are the steps we will follow:\n",
    "\n",
    "1. Load the data.\n",
    "2. Data cleaning/munging, feature engineering (will not do today)\n",
    "3. Make test/train splits. (Should we use cross validation?)\n",
    "4. Standardize the data.\n",
    "5. Build the computational graph for the neural network.\n",
    "6. Train the network using gradient descent a.k.a. back propogation.\n",
    "7. Evaluate performance and iterate.\n",
    "\n",
    "For regression, we will have one output unit (neuron) with _no_ activation function.  The value of this outpu unit will be prediction of the network given whatever input values went into the input layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps 1-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "data = fetch_california_housing()\n",
    "# data.data is the data matrix (input features)\n",
    "# data.target is the label vector\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target)\n",
    "ss = StandardScaler()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=y_train.reshape(-1,1)\n",
    "y_test=y_test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Build the network/graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5160, 8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape #placeholder step:Our network will save a set of instructions for what types of computations to do\n",
    "#For this to work, the computer needs to know what the data will look like\n",
    "#i.e. will it be a 2D array, or 1D?, how many rows? etc.\n",
    "#the placeholder layer provides this information for the network\n",
    "#without actually passing in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15480, 8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15480, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a place holder , have to specify the type\n",
    "tf.reset_default_graph() # reset the graph\n",
    "\n",
    "X = tf.placeholder(dtype=tf.float32, \n",
    "                   shape=(None,X_train.shape[1]), # set the shape as shape \n",
    "                   name='X')\n",
    "y = tf.placeholder(dtype=tf.float32,\n",
    "                   shape=(None),\n",
    "                   name='y')\n",
    "#setting up the hidden \n",
    "h1=tf.layers.dense(X,8,activation=tf.nn.relu,name='hidden1') # lowercase #activation function\n",
    "y_hat=tf.layers.dense(h1,1,activation=None,name='y_hat')\n",
    "\n",
    "loss =tf.losses.mean_squared_error(y,y_hat)\n",
    "optimizer=tf.train.AdamOptimizer(0.1)\n",
    "\n",
    "training_run=optimizer.minimize(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 train loss 2.1022387 test loss 2.0575085\n",
      "epoch 1 train loss 1.1596892 test loss 1.1822075\n",
      "epoch 2 train loss 1.2396127 test loss 1.2007258\n",
      "epoch 3 train loss 1.3814819 test loss 1.2530305\n",
      "epoch 4 train loss 1.1900164 test loss 1.0663164\n",
      "epoch 5 train loss 0.93822044 test loss 0.87662685\n",
      "epoch 6 train loss 0.7839898 test loss 0.7839278\n",
      "epoch 7 train loss 0.7140315 test loss 0.7365127\n",
      "epoch 8 train loss 0.6959278 test loss 0.7083939\n",
      "epoch 9 train loss 0.7242731 test loss 0.7172313\n",
      "epoch 10 train loss 0.76511896 test loss 0.73228383\n",
      "epoch 11 train loss 0.7759912 test loss 0.72221434\n",
      "epoch 12 train loss 0.74108326 test loss 0.68379647\n",
      "epoch 13 train loss 0.6771135 test loss 0.6371141\n",
      "epoch 14 train loss 0.6060588 test loss 0.5927881\n",
      "epoch 15 train loss 0.55431163 test loss 0.5592253\n",
      "epoch 16 train loss 0.53357774 test loss 0.5447464\n",
      "epoch 17 train loss 0.53809035 test loss 0.5443949\n",
      "epoch 18 train loss 0.55299366 test loss 0.5490985\n",
      "epoch 19 train loss 0.5651873 test loss 0.5519407\n",
      "epoch 20 train loss 0.5681295 test loss 0.5510124\n",
      "epoch 21 train loss 0.561575 test loss 0.54605484\n",
      "epoch 22 train loss 0.54645604 test loss 0.5363789\n",
      "epoch 23 train loss 0.5259579 test loss 0.5230115\n",
      "epoch 24 train loss 0.50625783 test loss 0.5101343\n",
      "epoch 25 train loss 0.49330872 test loss 0.50244606\n",
      "epoch 26 train loss 0.48858014 test loss 0.5007666\n",
      "epoch 27 train loss 0.4889215 test loss 0.50200677\n",
      "epoch 28 train loss 0.4901096 test loss 0.5021351\n",
      "epoch 29 train loss 0.48992997 test loss 0.4997379\n",
      "epoch 30 train loss 0.4878317 test loss 0.4951961\n",
      "epoch 31 train loss 0.48330146 test loss 0.48894468\n",
      "epoch 32 train loss 0.47614998 test loss 0.481295\n",
      "epoch 33 train loss 0.4676892 test loss 0.47337875\n",
      "epoch 34 train loss 0.46030626 test loss 0.4670528\n",
      "epoch 35 train loss 0.45537364 test loss 0.46285367\n",
      "epoch 36 train loss 0.45241156 test loss 0.45982927\n",
      "epoch 37 train loss 0.45056114 test loss 0.45706597\n",
      "epoch 38 train loss 0.4495969 test loss 0.45442253\n",
      "epoch 39 train loss 0.44880232 test loss 0.4538762\n",
      "epoch 40 train loss 0.44718927 test loss 0.45266217\n",
      "epoch 41 train loss 0.44444376 test loss 0.45087078\n",
      "epoch 42 train loss 0.44151485 test loss 0.44855145\n",
      "epoch 43 train loss 0.43902674 test loss 0.44588274\n",
      "epoch 44 train loss 0.43712187 test loss 0.44310373\n",
      "epoch 45 train loss 0.4357381 test loss 0.4411237\n",
      "epoch 46 train loss 0.43503425 test loss 0.43950838\n",
      "epoch 47 train loss 0.43448022 test loss 0.43809304\n",
      "epoch 48 train loss 0.43323725 test loss 0.43645018\n",
      "epoch 49 train loss 0.4311512 test loss 0.43449512\n",
      "epoch 50 train loss 0.4287719 test loss 0.43251914\n",
      "epoch 51 train loss 0.42651922 test loss 0.43062234\n",
      "epoch 52 train loss 0.42468962 test loss 0.4289071\n",
      "epoch 53 train loss 0.42356277 test loss 0.4278554\n",
      "epoch 54 train loss 0.4230485 test loss 0.42753088\n",
      "epoch 55 train loss 0.42252716 test loss 0.4273551\n",
      "epoch 56 train loss 0.42156854 test loss 0.42681694\n",
      "epoch 57 train loss 0.42021158 test loss 0.42573914\n",
      "epoch 58 train loss 0.41858655 test loss 0.4241349\n",
      "epoch 59 train loss 0.4169462 test loss 0.4221215\n",
      "epoch 60 train loss 0.41565654 test loss 0.42030016\n",
      "epoch 61 train loss 0.41483113 test loss 0.419091\n",
      "epoch 62 train loss 0.4141954 test loss 0.41837886\n",
      "epoch 63 train loss 0.4134775 test loss 0.4177825\n",
      "epoch 64 train loss 0.41258937 test loss 0.4170955\n",
      "epoch 65 train loss 0.41152528 test loss 0.41618747\n",
      "epoch 66 train loss 0.41037035 test loss 0.4150824\n",
      "epoch 67 train loss 0.4093936 test loss 0.41413575\n",
      "epoch 68 train loss 0.4086612 test loss 0.41351053\n",
      "epoch 69 train loss 0.40803793 test loss 0.4131683\n",
      "epoch 70 train loss 0.40740788 test loss 0.41290188\n",
      "epoch 71 train loss 0.40669438 test loss 0.41252133\n",
      "epoch 72 train loss 0.40585464 test loss 0.41188827\n",
      "epoch 73 train loss 0.405037 test loss 0.41112807\n",
      "epoch 74 train loss 0.4044156 test loss 0.41045925\n",
      "epoch 75 train loss 0.40393198 test loss 0.40993464\n",
      "epoch 76 train loss 0.40351894 test loss 0.4095913\n",
      "epoch 77 train loss 0.40308753 test loss 0.40930343\n",
      "epoch 78 train loss 0.40257004 test loss 0.40894133\n",
      "epoch 79 train loss 0.40201166 test loss 0.40852076\n",
      "epoch 80 train loss 0.40152586 test loss 0.40820143\n",
      "epoch 81 train loss 0.4011259 test loss 0.40798992\n",
      "epoch 82 train loss 0.4007618 test loss 0.40779132\n",
      "epoch 83 train loss 0.40039226 test loss 0.4075042\n",
      "epoch 84 train loss 0.3999918 test loss 0.4070516\n",
      "epoch 85 train loss 0.39956996 test loss 0.4064791\n",
      "epoch 86 train loss 0.39918193 test loss 0.405945\n",
      "epoch 87 train loss 0.39883116 test loss 0.40554196\n",
      "epoch 88 train loss 0.39850068 test loss 0.40525556\n",
      "epoch 89 train loss 0.39815533 test loss 0.4050008\n",
      "epoch 90 train loss 0.39777574 test loss 0.40470117\n",
      "epoch 91 train loss 0.39738783 test loss 0.4043582\n",
      "epoch 92 train loss 0.39702147 test loss 0.40401247\n",
      "epoch 93 train loss 0.3966812 test loss 0.40369296\n",
      "epoch 94 train loss 0.39635316 test loss 0.40336177\n",
      "epoch 95 train loss 0.39601928 test loss 0.4029608\n",
      "epoch 96 train loss 0.39567092 test loss 0.4024731\n",
      "epoch 97 train loss 0.39532018 test loss 0.4019623\n",
      "epoch 98 train loss 0.3949857 test loss 0.4014916\n",
      "epoch 99 train loss 0.39465818 test loss 0.40109494\n",
      "epoch 100 train loss 0.39432004 test loss 0.40073758\n",
      "epoch 101 train loss 0.393965 test loss 0.40035284\n",
      "epoch 102 train loss 0.39361015 test loss 0.39992988\n",
      "epoch 103 train loss 0.39326444 test loss 0.399543\n",
      "epoch 104 train loss 0.39293197 test loss 0.39921847\n",
      "epoch 105 train loss 0.39259845 test loss 0.39892513\n",
      "epoch 106 train loss 0.39224502 test loss 0.398659\n",
      "epoch 107 train loss 0.39187926 test loss 0.3983632\n",
      "epoch 108 train loss 0.39151034 test loss 0.39804587\n",
      "epoch 109 train loss 0.39114627 test loss 0.39769435\n",
      "epoch 110 train loss 0.39078218 test loss 0.39729175\n",
      "epoch 111 train loss 0.3904117 test loss 0.39693668\n",
      "epoch 112 train loss 0.39003772 test loss 0.39659688\n",
      "epoch 113 train loss 0.38966358 test loss 0.3962192\n",
      "epoch 114 train loss 0.38929608 test loss 0.3958041\n",
      "epoch 115 train loss 0.38892436 test loss 0.3953627\n",
      "epoch 116 train loss 0.38854447 test loss 0.39489084\n",
      "epoch 117 train loss 0.3881657 test loss 0.39437947\n",
      "epoch 118 train loss 0.38777122 test loss 0.3938872\n",
      "epoch 119 train loss 0.38736475 test loss 0.39344525\n",
      "epoch 120 train loss 0.3869357 test loss 0.39302438\n",
      "epoch 121 train loss 0.3864864 test loss 0.39265364\n",
      "epoch 122 train loss 0.38603193 test loss 0.39237306\n",
      "epoch 123 train loss 0.3855332 test loss 0.39202318\n",
      "epoch 124 train loss 0.38503453 test loss 0.39167607\n",
      "epoch 125 train loss 0.38450742 test loss 0.391402\n",
      "epoch 126 train loss 0.38397148 test loss 0.3910533\n",
      "epoch 127 train loss 0.38341054 test loss 0.39056784\n",
      "epoch 128 train loss 0.3828329 test loss 0.39002848\n",
      "epoch 129 train loss 0.38223383 test loss 0.38953942\n",
      "epoch 130 train loss 0.3816337 test loss 0.3890371\n",
      "epoch 131 train loss 0.3810185 test loss 0.388518\n",
      "epoch 132 train loss 0.38039076 test loss 0.3881103\n",
      "epoch 133 train loss 0.37975937 test loss 0.38777375\n",
      "epoch 134 train loss 0.37911943 test loss 0.38733822\n",
      "epoch 135 train loss 0.37847018 test loss 0.3867665\n",
      "epoch 136 train loss 0.37781557 test loss 0.38613576\n",
      "epoch 137 train loss 0.3771447 test loss 0.38563862\n",
      "epoch 138 train loss 0.37646922 test loss 0.38529372\n",
      "epoch 139 train loss 0.37578216 test loss 0.38500077\n",
      "epoch 140 train loss 0.375081 test loss 0.3846089\n",
      "epoch 141 train loss 0.37438124 test loss 0.38404632\n",
      "epoch 142 train loss 0.3736849 test loss 0.3834782\n",
      "epoch 143 train loss 0.37301177 test loss 0.38296854\n",
      "epoch 144 train loss 0.3723479 test loss 0.38238633\n",
      "epoch 145 train loss 0.37168851 test loss 0.3816785\n",
      "epoch 146 train loss 0.371031 test loss 0.38095334\n",
      "epoch 147 train loss 0.37039533 test loss 0.38025793\n",
      "epoch 148 train loss 0.36977327 test loss 0.37952977\n",
      "epoch 149 train loss 0.36915514 test loss 0.37879342\n",
      "epoch 150 train loss 0.36854452 test loss 0.37816447\n",
      "epoch 151 train loss 0.36795354 test loss 0.3776552\n",
      "epoch 152 train loss 0.3673834 test loss 0.3772765\n",
      "epoch 153 train loss 0.36681548 test loss 0.37710217\n",
      "epoch 154 train loss 0.36626124 test loss 0.37692815\n",
      "epoch 155 train loss 0.36573204 test loss 0.37683964\n",
      "epoch 156 train loss 0.3652334 test loss 0.37678856\n",
      "epoch 157 train loss 0.36474976 test loss 0.37639984\n",
      "epoch 158 train loss 0.3642722 test loss 0.37579283\n",
      "epoch 159 train loss 0.3638176 test loss 0.37522498\n",
      "epoch 160 train loss 0.36338723 test loss 0.37468237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 161 train loss 0.36297232 test loss 0.37403262\n",
      "epoch 162 train loss 0.3625738 test loss 0.37374952\n",
      "epoch 163 train loss 0.36218122 test loss 0.3732648\n",
      "epoch 164 train loss 0.361812 test loss 0.37268072\n",
      "epoch 165 train loss 0.36145675 test loss 0.3720692\n",
      "epoch 166 train loss 0.36111578 test loss 0.37176725\n",
      "epoch 167 train loss 0.36077762 test loss 0.37156177\n",
      "epoch 168 train loss 0.36044317 test loss 0.37117592\n",
      "epoch 169 train loss 0.36011234 test loss 0.37076408\n",
      "epoch 170 train loss 0.35978067 test loss 0.3705982\n",
      "epoch 171 train loss 0.3594462 test loss 0.37038985\n",
      "epoch 172 train loss 0.3591257 test loss 0.37025398\n",
      "epoch 173 train loss 0.35880697 test loss 0.37029412\n",
      "epoch 174 train loss 0.35849926 test loss 0.36979452\n",
      "epoch 175 train loss 0.3581929 test loss 0.36926278\n",
      "epoch 176 train loss 0.357893 test loss 0.3689547\n",
      "epoch 177 train loss 0.3575992 test loss 0.36848164\n",
      "epoch 178 train loss 0.35730484 test loss 0.36791912\n",
      "epoch 179 train loss 0.3570103 test loss 0.36764303\n",
      "epoch 180 train loss 0.35671982 test loss 0.36776522\n",
      "epoch 181 train loss 0.35645372 test loss 0.3677409\n",
      "epoch 182 train loss 0.35621023 test loss 0.3681227\n",
      "epoch 183 train loss 0.35595906 test loss 0.36737785\n",
      "epoch 184 train loss 0.35570374 test loss 0.36731148\n",
      "epoch 185 train loss 0.35547164 test loss 0.36741605\n",
      "epoch 186 train loss 0.35523868 test loss 0.36710185\n",
      "epoch 187 train loss 0.35501432 test loss 0.3671561\n",
      "epoch 188 train loss 0.35478497 test loss 0.36665443\n",
      "epoch 189 train loss 0.35455278 test loss 0.36619025\n",
      "epoch 190 train loss 0.35432753 test loss 0.36607194\n",
      "epoch 191 train loss 0.35410312 test loss 0.3660706\n",
      "epoch 192 train loss 0.35388222 test loss 0.36516246\n",
      "epoch 193 train loss 0.35365498 test loss 0.365467\n",
      "epoch 194 train loss 0.35342833 test loss 0.3645744\n",
      "epoch 195 train loss 0.35322985 test loss 0.36505497\n",
      "epoch 196 train loss 0.35302973 test loss 0.36410642\n",
      "epoch 197 train loss 0.3528331 test loss 0.36445674\n",
      "epoch 198 train loss 0.35264543 test loss 0.3638517\n",
      "epoch 199 train loss 0.35247415 test loss 0.36430132\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "train_errs = []\n",
    "test_errs = []\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for epoch in range(200):\n",
    "        sess.run(training_run, feed_dict={X: X_train, y: y_train})\n",
    "        \n",
    "        # Calculate train loss\n",
    "        train_loss = sess.run(loss, feed_dict={X: X_train, y: y_train})\n",
    "        train_errs.append(train_loss)\n",
    "        \n",
    "        # Calculate test loss\n",
    "        test_loss = sess.run(loss, feed_dict={X: X_test, y: y_test})\n",
    "        test_errs.append(test_loss)\n",
    "        \n",
    "        # Print losses\n",
    "        print('epoch', epoch, 'train loss', train_loss, 'test loss', test_loss)\n",
    "    pred = sess.run(y_hat, feed_dict={X: X_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the learning curves\n",
    "plt.plot(train_errs, label='Train loss')\n",
    "plt.plot(test_errs, label='Test loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7365309223358643"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate r^2\n",
    "metrics.r2_score(y_test, pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curves\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Neural Network for Binary Classification\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll build a neural net for _classification_.  We will follow the same steps as for regression:\n",
    "\n",
    "1. Load the data.\n",
    "2. Data cleaning/munging, feature engineering (will not do today)\n",
    "3. Make test/train splits. (Should we use cross validation?)\n",
    "4. Standardize the data.\n",
    "5. Build the computational graph for the neural network.\n",
    "6. Train the network using gradient descent a.k.a. back propogation.\n",
    "7. Evaluate performance and iterate.\n",
    "\n",
    "For _binary classification_ we will have one output unit that will represent the **probability** of \"class 1.\"  Because we want a probability as output, we need to select an activation function that yields values between 0 and 1, i.e., sigmoid function or similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "data = make_classification(n_samples=10000, n_features=20, random_state=42)\n",
    "# in data.target...\n",
    "#  1 = benign\n",
    "#  0 = malignant\n",
    "features, labels = make_classification(n_samples=10000, n_features=20, random_state=42)\n",
    "# in data.target...\n",
    "#  1 = benign\n",
    "#  0 = malignant\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels)\n",
    "ss= StandardScaler()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=y_train.reshape(-1,1)\n",
    "y_test=y_test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(dtype=tf.float32, shape=(None, X_train.shape[1]), name='X')\n",
    "y = tf.placeholder(dtype=tf.float32, shape=(None, 1), name='y')\n",
    "\n",
    "h1 = tf.layers.dense(X, 20, activation=tf.nn.relu, name='hidden1')\n",
    "# adding another layer\n",
    "h2 = tf.layers.dense(h1, 10, activation=tf.nn.relu, name='hidden2')\n",
    "y_hat = tf.layers.dense(h2, 1, activation=tf.nn.sigmoid, name='y_hat')\n",
    "\n",
    "loss = tf.losses.log_loss(y, y_hat)\n",
    "optimizer = tf.train.AdamOptimizer(.01)\n",
    "training_run = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 train loss 0.73798627 test loss 0.740544\n",
      "epoch 10 train loss 0.44768167 test loss 0.4714464\n",
      "epoch 20 train loss 0.3041889 test loss 0.3337019\n",
      "epoch 30 train loss 0.28197545 test loss 0.30885085\n",
      "epoch 40 train loss 0.25892663 test loss 0.28745264\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(50):\n",
    "        sess.run(training_run, feed_dict={X: X_train, y: y_train})\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            train_loss = sess.run(loss, feed_dict={X: X_train, y: y_train})\n",
    "            test_loss = sess.run(loss, feed_dict={X: X_test, y: y_test})\n",
    "            print('epoch', epoch, 'train loss', train_loss, 'test loss', test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, let's go back and add a new hidden layer to our network..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the learning curves\n",
    "plt.plot(train_loss, label='Train loss')\n",
    "plt.plot(test_loss, label='Test loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching the input\n",
    "\n",
    "What if we can't store all of the training data in memory?\n",
    "\n",
    "We can split the data up into \"batches\" and feed them to the network one at a time.  This just means that we splitting up the data and feeding it to the network one piece at a time.\n",
    "\n",
    "You could write your own function to dole out subsets of the data one at a time, or you could use this creative hack to get `sklearn` to do it for you.  (Thanks, Riley, for showing me this!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "batches = round(X_train.shape[0] / batch_size)\n",
    "batches\n",
    "\n",
    "kf = KFold(n_splits=batches, shuffle=True)\n",
    "\n",
    "batch_list = []\n",
    "for train, test in kf.split(X_train, y_train):\n",
    "    batch_list.append(test_fold)\n",
    "    \n",
    "# batch_list = [y for _, y in kf.split(X_train, y_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "Saver=tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 train 0.25990275\n",
      "epoch 1 train 0.22426528\n",
      "epoch 2 train 0.21548931\n",
      "epoch 3 train 0.21087152\n",
      "epoch 4 train 0.20720808\n",
      "epoch 5 train 0.20385271\n",
      "epoch 6 train 0.20112607\n",
      "epoch 7 train 0.19824308\n",
      "epoch 8 train 0.19493145\n",
      "epoch 9 train 0.19278729\n",
      "epoch 10 train 0.18985556\n",
      "epoch 11 train 0.1865451\n",
      "epoch 12 train 0.1841415\n",
      "epoch 13 train 0.18207574\n",
      "epoch 14 train 0.17858686\n",
      "epoch 15 train 0.17523755\n",
      "epoch 16 train 0.1728377\n",
      "epoch 17 train 0.17040236\n",
      "epoch 18 train 0.1690107\n",
      "epoch 19 train 0.16739334\n",
      "epoch 20 train 0.16547397\n",
      "epoch 21 train 0.16367903\n",
      "epoch 22 train 0.16092895\n",
      "epoch 23 train 0.15953508\n",
      "epoch 24 train 0.15715726\n",
      "epoch 25 train 0.157494\n",
      "epoch 26 train 0.15654033\n",
      "epoch 27 train 0.15489145\n",
      "epoch 28 train 0.15465556\n",
      "epoch 29 train 0.15367319\n",
      "epoch 30 train 0.15200156\n",
      "epoch 31 train 0.15233368\n",
      "epoch 32 train 0.15054938\n",
      "epoch 33 train 0.15079094\n",
      "epoch 34 train 0.14976798\n",
      "epoch 35 train 0.15015757\n",
      "epoch 36 train 0.14941536\n",
      "epoch 37 train 0.14829178\n",
      "epoch 38 train 0.14747877\n",
      "epoch 39 train 0.14714986\n",
      "epoch 40 train 0.1466927\n",
      "epoch 41 train 0.14487676\n",
      "epoch 42 train 0.14518611\n",
      "epoch 43 train 0.14441007\n",
      "epoch 44 train 0.14176285\n",
      "epoch 45 train 0.14232081\n",
      "epoch 46 train 0.1410617\n",
      "epoch 47 train 0.14093885\n",
      "epoch 48 train 0.1401835\n",
      "epoch 49 train 0.1402783\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for epoch in range(50):\n",
    "        for batch in batch_list:\n",
    "            X_batch = X_train[batch]\n",
    "            y_batch = y_train[batch]\n",
    "            sess.run(training_run, feed_dict={X: X_batch, y: y_batch})\n",
    "        train_loss = sess.run(loss, feed_dict={X: X_train, y: y_train})\n",
    "        print('epoch', epoch, 'train', train_loss)\n",
    "    Saver.save(sess,'./classification.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./classification.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    Saver.restore(sess,'./classification.ckpt')\n",
    "    pred=sess.run(y_hat,feed_dict={X:X_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are youre actual predicted classes?\n",
    "classes=(pred>0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9196"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is your accuracy score?\n",
    "metrics.accuracy_score(y_test,classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1181,   98],\n",
       "       [ 103, 1118]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What about your confusion matrix?\n",
    "metrics.confusion_matrix(y_test,classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Neural Network for Multi-Class Classification\n",
    "\n",
    "---\n",
    "\n",
    "In _multi-class_ classification (i.e., more than 2 classes), we typically setup on output unit **per class** and use a \"softmax\" activation function to normalize all values between 0 and 1 so that they look like probabilities.  The output unit with the largest value corresponds to the class prediction.  (I.e., unit 3 had the highest value, so I will predict class 3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris()\n",
    "X_train,X_test,y_train,y_test=train_test_split(data.data,data.target)\n",
    "\n",
    "ss=StandardScaler()\n",
    "X_train=ss.fit_transform(X_train)\n",
    "X_test=ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=to_categorical(y_train)\n",
    "y_test=to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X=tf.placeholder(dtype=tf.float32,shape=(None,4),name='X')\n",
    "y=tf.placeholder(dtype=tf.float32,shape=(None,3),name='X')\n",
    "\n",
    "h1=tf.layers.dense(X,4,activation=tf.nn.relu, name='h1')\n",
    "y_hat=tf.layers.dense(h1,3,activation=None, name='y_hat') # soft_max is auto\n",
    "\n",
    "loss =tf.losses.softmax_cross_entropy(y,y_hat)\n",
    "optimizer=tf.train.AdamOptimizer(0.01)\n",
    "training_run=optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 train 1.0417844\n",
      "epoch 10 train 0.8250141\n",
      "epoch 20 train 0.6223268\n",
      "epoch 30 train 0.49209303\n",
      "epoch 40 train 0.42149705\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(50):\n",
    "        sess.run(training_run, feed_dict={X: X_train, y: y_train})\n",
    "        if epoch % 10 == 0:\n",
    "            train_loss = sess.run(loss, feed_dict={X: X_train, y: y_train})\n",
    "            print('epoch', epoch, 'train', train_loss)\n",
    "    pred = sess.run(y_hat, feed_dict={X: X_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, 0, 1, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 0, 2, 2, 0, 1,\n",
       "       0, 0, 0, 0, 0, 2, 2, 0, 1, 0, 2, 1, 2, 0, 2, 2])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# which class has the largest number\n",
    "pred.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each input this is the classification output per row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Your Model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Suggestions\n",
    "\n",
    "---\n",
    "\n",
    "- In binary classification, start with a single output node with a sigmoid activation function.\n",
    "- For multi-class classification, have one output node for each class and use the softmax activation function.\n",
    "- For hidden layers, the ReLU and hyperbolic tangent (tanh) activation functions often work well.  Start with the ReLU as your first trial.\n",
    "- Start with one hidden layer, then trying adding another if performance is not good.\n",
    "- For simplicity, start with the same number of units in each hidden layer, then increase this number for all hidden layers simultaneously if the performance is not good.\n",
    "- Alternatively, you can use the \"stretchy pants\" approach and through a lot of hidden nodes into your network, but then stop training as soon as you detect the onset of overfitting.\n",
    "\n",
    "> These suggestions are largely adapted from the book _Hands-on Machine Learning with Scikit-Learn & Tensorflow_ by Aurelien Geron, a book I highly recommend!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
