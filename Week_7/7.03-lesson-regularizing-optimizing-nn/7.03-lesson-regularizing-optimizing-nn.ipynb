{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\"\\>\n",
    "\n",
    "## Regularization and Optimization for Neural Networks\n",
    "\n",
    "---\n",
    "\n",
    "_Authors: Matt Brems and Justin Pounders (but mainly Matt)_\n",
    "\n",
    "The principal topic of the day is **how to control bias/variance tradeoff in neural nets.**\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of the lesson, students should be able to:\n",
    "- Explain how L1/L2, dropout, and early stopping regularization work and implement these methods in Keras\n",
    "- Implement methods for speeding up learning\n",
    "- Describe Gradient Descent with Momentum and Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>account_length</th>\n",
       "      <th>area_code</th>\n",
       "      <th>intl_plan</th>\n",
       "      <th>vmail_plan</th>\n",
       "      <th>vmail_message</th>\n",
       "      <th>day_mins</th>\n",
       "      <th>day_calls</th>\n",
       "      <th>day_charge</th>\n",
       "      <th>eve_mins</th>\n",
       "      <th>eve_calls</th>\n",
       "      <th>eve_charge</th>\n",
       "      <th>night_mins</th>\n",
       "      <th>night_calls</th>\n",
       "      <th>night_charge</th>\n",
       "      <th>intl_mins</th>\n",
       "      <th>intl_calls</th>\n",
       "      <th>intl_charge</th>\n",
       "      <th>custserv_calls</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KS</td>\n",
       "      <td>128</td>\n",
       "      <td>415</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>25</td>\n",
       "      <td>265.1</td>\n",
       "      <td>110</td>\n",
       "      <td>45.07</td>\n",
       "      <td>197.4</td>\n",
       "      <td>99</td>\n",
       "      <td>16.78</td>\n",
       "      <td>244.7</td>\n",
       "      <td>91</td>\n",
       "      <td>11.01</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.70</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OH</td>\n",
       "      <td>107</td>\n",
       "      <td>415</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>26</td>\n",
       "      <td>161.6</td>\n",
       "      <td>123</td>\n",
       "      <td>27.47</td>\n",
       "      <td>195.5</td>\n",
       "      <td>103</td>\n",
       "      <td>16.62</td>\n",
       "      <td>254.4</td>\n",
       "      <td>103</td>\n",
       "      <td>11.45</td>\n",
       "      <td>13.7</td>\n",
       "      <td>3</td>\n",
       "      <td>3.70</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NJ</td>\n",
       "      <td>137</td>\n",
       "      <td>415</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>243.4</td>\n",
       "      <td>114</td>\n",
       "      <td>41.38</td>\n",
       "      <td>121.2</td>\n",
       "      <td>110</td>\n",
       "      <td>10.30</td>\n",
       "      <td>162.6</td>\n",
       "      <td>104</td>\n",
       "      <td>7.32</td>\n",
       "      <td>12.2</td>\n",
       "      <td>5</td>\n",
       "      <td>3.29</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OH</td>\n",
       "      <td>84</td>\n",
       "      <td>408</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>299.4</td>\n",
       "      <td>71</td>\n",
       "      <td>50.90</td>\n",
       "      <td>61.9</td>\n",
       "      <td>88</td>\n",
       "      <td>5.26</td>\n",
       "      <td>196.9</td>\n",
       "      <td>89</td>\n",
       "      <td>8.86</td>\n",
       "      <td>6.6</td>\n",
       "      <td>7</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OK</td>\n",
       "      <td>75</td>\n",
       "      <td>415</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>166.7</td>\n",
       "      <td>113</td>\n",
       "      <td>28.34</td>\n",
       "      <td>148.3</td>\n",
       "      <td>122</td>\n",
       "      <td>12.61</td>\n",
       "      <td>186.9</td>\n",
       "      <td>121</td>\n",
       "      <td>8.41</td>\n",
       "      <td>10.1</td>\n",
       "      <td>3</td>\n",
       "      <td>2.73</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  state  account_length  area_code intl_plan vmail_plan  vmail_message  \\\n",
       "0    KS             128        415        no        yes             25   \n",
       "1    OH             107        415        no        yes             26   \n",
       "2    NJ             137        415        no         no              0   \n",
       "3    OH              84        408       yes         no              0   \n",
       "4    OK              75        415       yes         no              0   \n",
       "\n",
       "   day_mins  day_calls  day_charge  eve_mins  eve_calls  eve_charge  \\\n",
       "0     265.1        110       45.07     197.4         99       16.78   \n",
       "1     161.6        123       27.47     195.5        103       16.62   \n",
       "2     243.4        114       41.38     121.2        110       10.30   \n",
       "3     299.4         71       50.90      61.9         88        5.26   \n",
       "4     166.7        113       28.34     148.3        122       12.61   \n",
       "\n",
       "   night_mins  night_calls  night_charge  intl_mins  intl_calls  intl_charge  \\\n",
       "0       244.7           91         11.01       10.0           3         2.70   \n",
       "1       254.4          103         11.45       13.7           3         3.70   \n",
       "2       162.6          104          7.32       12.2           5         3.29   \n",
       "3       196.9           89          8.86        6.6           7         1.78   \n",
       "4       186.9          121          8.41       10.1           3         2.73   \n",
       "\n",
       "   custserv_calls  churn  \n",
       "0               1  False  \n",
       "1               1  False  \n",
       "2               0  False  \n",
       "3               2  False  \n",
       "4               3  False  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./data/cell_phone_churn.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3333 entries, 0 to 3332\n",
      "Data columns (total 19 columns):\n",
      "account_length    3333 non-null int64\n",
      "area_code         3333 non-null int64\n",
      "intl_plan         3333 non-null int64\n",
      "vmail_plan        3333 non-null int64\n",
      "vmail_message     3333 non-null int64\n",
      "day_mins          3333 non-null float64\n",
      "day_calls         3333 non-null int64\n",
      "day_charge        3333 non-null float64\n",
      "eve_mins          3333 non-null float64\n",
      "eve_calls         3333 non-null int64\n",
      "eve_charge        3333 non-null float64\n",
      "night_mins        3333 non-null float64\n",
      "night_calls       3333 non-null int64\n",
      "night_charge      3333 non-null float64\n",
      "intl_mins         3333 non-null float64\n",
      "intl_calls        3333 non-null int64\n",
      "intl_charge       3333 non-null float64\n",
      "custserv_calls    3333 non-null int64\n",
      "churn             3333 non-null bool\n",
      "dtypes: bool(1), float64(8), int64(10)\n",
      "memory usage: 472.0 KB\n"
     ]
    }
   ],
   "source": [
    "# A bit of house cleaning...\n",
    "\n",
    "# Drop state for simplicity in class\n",
    "data = data.drop('state', axis=1)\n",
    "\n",
    "# Map these yes/no values to 1/0\n",
    "data['intl_plan'] = data['intl_plan'].map(lambda x: 1 if x=='yes' else 0)\n",
    "data['vmail_plan'] = data['vmail_plan'].map(lambda x: 1 if x=='yes' else 0)\n",
    "\n",
    "# And see how we did...\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split, predicting churn\n",
    "X = data.drop('churn', axis=1)\n",
    "y = data['churn'].astype('int')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# Standardize\n",
    "ss = StandardScaler()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a simple neural net to model churn\n",
    "\n",
    "Let's build this:\n",
    "\n",
    "- a dense network,\n",
    "- one input layer,\n",
    "- one hidden layer \n",
    "  - same number of nodes as input layer,\n",
    "  - ReLU activation\n",
    "- single node output (for binary classification)\n",
    "  - sigmoid activation\n",
    "  \n",
    "> **Fun fact**: If we dropped the hidden layer, this model would just be logistic regression!  Can you prove that to yourself?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert model here...\n",
    "model=Sequential()\n",
    "n_input = X_train.shape[1]\n",
    "n_hidden=n_input\n",
    "n_output=1\n",
    "model.add(Dense(n_hidden,input_dim=n_input,activation='relu'))\n",
    "model.add(Dense(n_output,activation=('sigmoid')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile it\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', \n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2499 samples, validate on 834 samples\n",
      "Epoch 1/100\n",
      "2499/2499 [==============================] - 0s 178us/step - loss: 0.6324 - acc: 0.6719 - val_loss: 0.4940 - val_acc: 0.8609\n",
      "Epoch 2/100\n",
      "2499/2499 [==============================] - 0s 81us/step - loss: 0.4566 - acc: 0.8511 - val_loss: 0.3991 - val_acc: 0.8657\n",
      "Epoch 3/100\n",
      "2499/2499 [==============================] - 0s 72us/step - loss: 0.3886 - acc: 0.8543 - val_loss: 0.3578 - val_acc: 0.8669\n",
      "Epoch 4/100\n",
      "2499/2499 [==============================] - 0s 69us/step - loss: 0.3547 - acc: 0.8591 - val_loss: 0.3367 - val_acc: 0.8729\n",
      "Epoch 5/100\n",
      "2499/2499 [==============================] - 0s 70us/step - loss: 0.3336 - acc: 0.8643 - val_loss: 0.3232 - val_acc: 0.8753\n",
      "Epoch 6/100\n",
      "2499/2499 [==============================] - 0s 73us/step - loss: 0.3186 - acc: 0.8639 - val_loss: 0.3149 - val_acc: 0.8825\n",
      "Epoch 7/100\n",
      "2499/2499 [==============================] - 0s 66us/step - loss: 0.3068 - acc: 0.8703 - val_loss: 0.3078 - val_acc: 0.8873\n",
      "Epoch 8/100\n",
      "2499/2499 [==============================] - 0s 67us/step - loss: 0.2957 - acc: 0.8764 - val_loss: 0.3022 - val_acc: 0.8909\n",
      "Epoch 9/100\n",
      "2499/2499 [==============================] - 0s 67us/step - loss: 0.2859 - acc: 0.8820 - val_loss: 0.2968 - val_acc: 0.8909\n",
      "Epoch 10/100\n",
      "2499/2499 [==============================] - 0s 65us/step - loss: 0.2764 - acc: 0.8900 - val_loss: 0.2914 - val_acc: 0.8957\n",
      "Epoch 11/100\n",
      "2499/2499 [==============================] - 0s 68us/step - loss: 0.2680 - acc: 0.8940 - val_loss: 0.2870 - val_acc: 0.8993\n",
      "Epoch 12/100\n",
      "2499/2499 [==============================] - 0s 65us/step - loss: 0.2597 - acc: 0.8964 - val_loss: 0.2818 - val_acc: 0.9029\n",
      "Epoch 13/100\n",
      "2499/2499 [==============================] - 0s 67us/step - loss: 0.2528 - acc: 0.9048 - val_loss: 0.2784 - val_acc: 0.9029\n",
      "Epoch 14/100\n",
      "2499/2499 [==============================] - 0s 65us/step - loss: 0.2467 - acc: 0.9060 - val_loss: 0.2754 - val_acc: 0.9065\n",
      "Epoch 15/100\n",
      "2499/2499 [==============================] - 0s 66us/step - loss: 0.2407 - acc: 0.9104 - val_loss: 0.2725 - val_acc: 0.9053\n",
      "Epoch 16/100\n",
      "2499/2499 [==============================] - 0s 67us/step - loss: 0.2361 - acc: 0.9136 - val_loss: 0.2695 - val_acc: 0.9053\n",
      "Epoch 17/100\n",
      "2499/2499 [==============================] - 0s 69us/step - loss: 0.2324 - acc: 0.9180 - val_loss: 0.2679 - val_acc: 0.9077\n",
      "Epoch 18/100\n",
      "2499/2499 [==============================] - 0s 68us/step - loss: 0.2289 - acc: 0.9172 - val_loss: 0.2660 - val_acc: 0.9065\n",
      "Epoch 19/100\n",
      "2499/2499 [==============================] - 0s 72us/step - loss: 0.2258 - acc: 0.9192 - val_loss: 0.2654 - val_acc: 0.9065\n",
      "Epoch 20/100\n",
      "2499/2499 [==============================] - 0s 67us/step - loss: 0.2238 - acc: 0.9212 - val_loss: 0.2637 - val_acc: 0.9053\n",
      "Epoch 21/100\n",
      "2499/2499 [==============================] - 0s 73us/step - loss: 0.2211 - acc: 0.9204 - val_loss: 0.2640 - val_acc: 0.9065\n",
      "Epoch 22/100\n",
      "2499/2499 [==============================] - 0s 76us/step - loss: 0.2189 - acc: 0.9212 - val_loss: 0.2641 - val_acc: 0.9101\n",
      "Epoch 23/100\n",
      "2499/2499 [==============================] - 0s 75us/step - loss: 0.2176 - acc: 0.9224 - val_loss: 0.2627 - val_acc: 0.9125\n",
      "Epoch 24/100\n",
      "2499/2499 [==============================] - 0s 85us/step - loss: 0.2152 - acc: 0.9240 - val_loss: 0.2627 - val_acc: 0.9101\n",
      "Epoch 25/100\n",
      "2499/2499 [==============================] - 0s 78us/step - loss: 0.2140 - acc: 0.9240 - val_loss: 0.2629 - val_acc: 0.9113\n",
      "Epoch 26/100\n",
      "2499/2499 [==============================] - 0s 78us/step - loss: 0.2122 - acc: 0.9252 - val_loss: 0.2621 - val_acc: 0.9125\n",
      "Epoch 27/100\n",
      "2499/2499 [==============================] - 0s 72us/step - loss: 0.2110 - acc: 0.9268 - val_loss: 0.2615 - val_acc: 0.9125\n",
      "Epoch 28/100\n",
      "2499/2499 [==============================] - 0s 72us/step - loss: 0.2099 - acc: 0.9264 - val_loss: 0.2616 - val_acc: 0.9125\n",
      "Epoch 29/100\n",
      "2499/2499 [==============================] - 0s 69us/step - loss: 0.2089 - acc: 0.9268 - val_loss: 0.2600 - val_acc: 0.9137\n",
      "Epoch 30/100\n",
      "2499/2499 [==============================] - 0s 126us/step - loss: 0.2076 - acc: 0.9268 - val_loss: 0.2608 - val_acc: 0.9113\n",
      "Epoch 31/100\n",
      "2499/2499 [==============================] - 0s 96us/step - loss: 0.2068 - acc: 0.9268 - val_loss: 0.2600 - val_acc: 0.9137\n",
      "Epoch 32/100\n",
      "2499/2499 [==============================] - 0s 114us/step - loss: 0.2057 - acc: 0.9304 - val_loss: 0.2597 - val_acc: 0.9137\n",
      "Epoch 33/100\n",
      "2499/2499 [==============================] - 0s 91us/step - loss: 0.2045 - acc: 0.9304 - val_loss: 0.2588 - val_acc: 0.9137\n",
      "Epoch 34/100\n",
      "2499/2499 [==============================] - 0s 118us/step - loss: 0.2035 - acc: 0.9304 - val_loss: 0.2595 - val_acc: 0.9101\n",
      "Epoch 35/100\n",
      "2499/2499 [==============================] - 0s 75us/step - loss: 0.2027 - acc: 0.9312 - val_loss: 0.2588 - val_acc: 0.9137\n",
      "Epoch 36/100\n",
      "2499/2499 [==============================] - 0s 75us/step - loss: 0.2020 - acc: 0.9308 - val_loss: 0.2581 - val_acc: 0.9149\n",
      "Epoch 37/100\n",
      "2499/2499 [==============================] - 0s 70us/step - loss: 0.2016 - acc: 0.9304 - val_loss: 0.2571 - val_acc: 0.9197\n",
      "Epoch 38/100\n",
      "2499/2499 [==============================] - 0s 71us/step - loss: 0.2009 - acc: 0.9344 - val_loss: 0.2573 - val_acc: 0.9149\n",
      "Epoch 39/100\n",
      "2499/2499 [==============================] - 0s 68us/step - loss: 0.2006 - acc: 0.9328 - val_loss: 0.2565 - val_acc: 0.9161\n",
      "Epoch 40/100\n",
      "2499/2499 [==============================] - 0s 73us/step - loss: 0.2000 - acc: 0.9324 - val_loss: 0.2549 - val_acc: 0.9185\n",
      "Epoch 41/100\n",
      "2499/2499 [==============================] - 0s 68us/step - loss: 0.1988 - acc: 0.9348 - val_loss: 0.2553 - val_acc: 0.9209\n",
      "Epoch 42/100\n",
      "2499/2499 [==============================] - 0s 70us/step - loss: 0.1989 - acc: 0.9328 - val_loss: 0.2545 - val_acc: 0.9197\n",
      "Epoch 43/100\n",
      "2499/2499 [==============================] - 0s 74us/step - loss: 0.1978 - acc: 0.9324 - val_loss: 0.2548 - val_acc: 0.9197\n",
      "Epoch 44/100\n",
      "2499/2499 [==============================] - 0s 66us/step - loss: 0.1970 - acc: 0.9344 - val_loss: 0.2555 - val_acc: 0.9197\n",
      "Epoch 45/100\n",
      "2499/2499 [==============================] - 0s 74us/step - loss: 0.1966 - acc: 0.9376 - val_loss: 0.2541 - val_acc: 0.9197\n",
      "Epoch 46/100\n",
      "2499/2499 [==============================] - 0s 70us/step - loss: 0.1956 - acc: 0.9344 - val_loss: 0.2531 - val_acc: 0.9197\n",
      "Epoch 47/100\n",
      "2499/2499 [==============================] - 0s 69us/step - loss: 0.1949 - acc: 0.9364 - val_loss: 0.2533 - val_acc: 0.9209\n",
      "Epoch 48/100\n",
      "2499/2499 [==============================] - 0s 69us/step - loss: 0.1943 - acc: 0.9340 - val_loss: 0.2529 - val_acc: 0.9197\n",
      "Epoch 49/100\n",
      "2499/2499 [==============================] - 0s 67us/step - loss: 0.1935 - acc: 0.9372 - val_loss: 0.2529 - val_acc: 0.9197\n",
      "Epoch 50/100\n",
      "2499/2499 [==============================] - 0s 66us/step - loss: 0.1929 - acc: 0.9364 - val_loss: 0.2525 - val_acc: 0.9209\n",
      "Epoch 51/100\n",
      "2499/2499 [==============================] - 0s 141us/step - loss: 0.1928 - acc: 0.9356 - val_loss: 0.2510 - val_acc: 0.9221\n",
      "Epoch 52/100\n",
      "2499/2499 [==============================] - 0s 94us/step - loss: 0.1919 - acc: 0.9376 - val_loss: 0.2522 - val_acc: 0.9221\n",
      "Epoch 53/100\n",
      "2499/2499 [==============================] - 0s 73us/step - loss: 0.1914 - acc: 0.9372 - val_loss: 0.2511 - val_acc: 0.9209\n",
      "Epoch 54/100\n",
      "2499/2499 [==============================] - 0s 73us/step - loss: 0.1910 - acc: 0.9364 - val_loss: 0.2511 - val_acc: 0.9245\n",
      "Epoch 55/100\n",
      "2499/2499 [==============================] - 0s 69us/step - loss: 0.1905 - acc: 0.9368 - val_loss: 0.2505 - val_acc: 0.9245\n",
      "Epoch 56/100\n",
      "2499/2499 [==============================] - 0s 68us/step - loss: 0.1897 - acc: 0.9368 - val_loss: 0.2501 - val_acc: 0.9233\n",
      "Epoch 57/100\n",
      "2499/2499 [==============================] - 0s 96us/step - loss: 0.1895 - acc: 0.9388 - val_loss: 0.2500 - val_acc: 0.9233\n",
      "Epoch 58/100\n",
      "2499/2499 [==============================] - 0s 94us/step - loss: 0.1895 - acc: 0.9360 - val_loss: 0.2508 - val_acc: 0.9245\n",
      "Epoch 59/100\n",
      "2499/2499 [==============================] - 0s 84us/step - loss: 0.1883 - acc: 0.9364 - val_loss: 0.2502 - val_acc: 0.9233\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2499/2499 [==============================] - 0s 81us/step - loss: 0.1881 - acc: 0.9368 - val_loss: 0.2502 - val_acc: 0.9209\n",
      "Epoch 61/100\n",
      "2499/2499 [==============================] - 0s 78us/step - loss: 0.1875 - acc: 0.9388 - val_loss: 0.2495 - val_acc: 0.9233\n",
      "Epoch 62/100\n",
      "2499/2499 [==============================] - 0s 83us/step - loss: 0.1869 - acc: 0.9408 - val_loss: 0.2493 - val_acc: 0.9233\n",
      "Epoch 63/100\n",
      "2499/2499 [==============================] - 0s 70us/step - loss: 0.1864 - acc: 0.9376 - val_loss: 0.2487 - val_acc: 0.9245\n",
      "Epoch 64/100\n",
      "2499/2499 [==============================] - 0s 69us/step - loss: 0.1868 - acc: 0.9396 - val_loss: 0.2486 - val_acc: 0.9245\n",
      "Epoch 65/100\n",
      "2499/2499 [==============================] - 0s 65us/step - loss: 0.1858 - acc: 0.9396 - val_loss: 0.2481 - val_acc: 0.9245\n",
      "Epoch 66/100\n",
      "2499/2499 [==============================] - 0s 66us/step - loss: 0.1853 - acc: 0.9400 - val_loss: 0.2480 - val_acc: 0.9257\n",
      "Epoch 67/100\n",
      "2499/2499 [==============================] - 0s 65us/step - loss: 0.1846 - acc: 0.9416 - val_loss: 0.2478 - val_acc: 0.9257\n",
      "Epoch 68/100\n",
      "2499/2499 [==============================] - 0s 66us/step - loss: 0.1844 - acc: 0.9392 - val_loss: 0.2493 - val_acc: 0.9245\n",
      "Epoch 69/100\n",
      "2499/2499 [==============================] - 0s 66us/step - loss: 0.1843 - acc: 0.9400 - val_loss: 0.2479 - val_acc: 0.9245\n",
      "Epoch 70/100\n",
      "2499/2499 [==============================] - 0s 66us/step - loss: 0.1835 - acc: 0.9400 - val_loss: 0.2473 - val_acc: 0.9269\n",
      "Epoch 71/100\n",
      "2499/2499 [==============================] - 0s 85us/step - loss: 0.1830 - acc: 0.9384 - val_loss: 0.2483 - val_acc: 0.9281\n",
      "Epoch 72/100\n",
      "2499/2499 [==============================] - 0s 64us/step - loss: 0.1827 - acc: 0.9400 - val_loss: 0.2475 - val_acc: 0.9281\n",
      "Epoch 73/100\n",
      "2499/2499 [==============================] - 0s 77us/step - loss: 0.1821 - acc: 0.9396 - val_loss: 0.2474 - val_acc: 0.9281\n",
      "Epoch 74/100\n",
      "2499/2499 [==============================] - 0s 122us/step - loss: 0.1824 - acc: 0.9392 - val_loss: 0.2474 - val_acc: 0.9257\n",
      "Epoch 75/100\n",
      "2499/2499 [==============================] - 0s 80us/step - loss: 0.1821 - acc: 0.9420 - val_loss: 0.2456 - val_acc: 0.9269\n",
      "Epoch 76/100\n",
      "2499/2499 [==============================] - 0s 63us/step - loss: 0.1815 - acc: 0.9404 - val_loss: 0.2465 - val_acc: 0.9281\n",
      "Epoch 77/100\n",
      "2499/2499 [==============================] - 0s 67us/step - loss: 0.1811 - acc: 0.9392 - val_loss: 0.2468 - val_acc: 0.9293\n",
      "Epoch 78/100\n",
      "2499/2499 [==============================] - 0s 63us/step - loss: 0.1805 - acc: 0.9416 - val_loss: 0.2473 - val_acc: 0.9269\n",
      "Epoch 79/100\n",
      "2499/2499 [==============================] - 0s 63us/step - loss: 0.1798 - acc: 0.9440 - val_loss: 0.2472 - val_acc: 0.9269\n",
      "Epoch 80/100\n",
      "2499/2499 [==============================] - 0s 60us/step - loss: 0.1797 - acc: 0.9412 - val_loss: 0.2477 - val_acc: 0.9257\n",
      "Epoch 81/100\n",
      "2499/2499 [==============================] - 0s 61us/step - loss: 0.1797 - acc: 0.9420 - val_loss: 0.2476 - val_acc: 0.9257\n",
      "Epoch 82/100\n",
      "2499/2499 [==============================] - 0s 63us/step - loss: 0.1795 - acc: 0.9432 - val_loss: 0.2474 - val_acc: 0.9281\n",
      "Epoch 83/100\n",
      "2499/2499 [==============================] - 0s 63us/step - loss: 0.1788 - acc: 0.9412 - val_loss: 0.2475 - val_acc: 0.9281\n",
      "Epoch 84/100\n",
      "2499/2499 [==============================] - 0s 65us/step - loss: 0.1784 - acc: 0.9424 - val_loss: 0.2476 - val_acc: 0.9269\n",
      "Epoch 85/100\n",
      "2499/2499 [==============================] - 0s 64us/step - loss: 0.1784 - acc: 0.9436 - val_loss: 0.2470 - val_acc: 0.9281\n",
      "Epoch 86/100\n",
      "2499/2499 [==============================] - 0s 63us/step - loss: 0.1785 - acc: 0.9420 - val_loss: 0.2462 - val_acc: 0.9281\n",
      "Epoch 87/100\n",
      "2499/2499 [==============================] - 0s 68us/step - loss: 0.1779 - acc: 0.9428 - val_loss: 0.2467 - val_acc: 0.9293\n",
      "Epoch 88/100\n",
      "2499/2499 [==============================] - 0s 65us/step - loss: 0.1771 - acc: 0.9420 - val_loss: 0.2471 - val_acc: 0.9281\n",
      "Epoch 89/100\n",
      "2499/2499 [==============================] - 0s 78us/step - loss: 0.1773 - acc: 0.9432 - val_loss: 0.2470 - val_acc: 0.9269\n",
      "Epoch 90/100\n",
      "2499/2499 [==============================] - 0s 82us/step - loss: 0.1769 - acc: 0.9428 - val_loss: 0.2471 - val_acc: 0.9269\n",
      "Epoch 91/100\n",
      "2499/2499 [==============================] - 0s 94us/step - loss: 0.1761 - acc: 0.9428 - val_loss: 0.2473 - val_acc: 0.9257\n",
      "Epoch 92/100\n",
      "2499/2499 [==============================] - 0s 68us/step - loss: 0.1763 - acc: 0.9428 - val_loss: 0.2475 - val_acc: 0.9269\n",
      "Epoch 93/100\n",
      "2499/2499 [==============================] - 0s 75us/step - loss: 0.1760 - acc: 0.9432 - val_loss: 0.2469 - val_acc: 0.9257\n",
      "Epoch 94/100\n",
      "2499/2499 [==============================] - 0s 66us/step - loss: 0.1754 - acc: 0.9444 - val_loss: 0.2464 - val_acc: 0.9269\n",
      "Epoch 95/100\n",
      "2499/2499 [==============================] - 0s 65us/step - loss: 0.1754 - acc: 0.9452 - val_loss: 0.2464 - val_acc: 0.9257\n",
      "Epoch 96/100\n",
      "2499/2499 [==============================] - 0s 75us/step - loss: 0.1750 - acc: 0.9436 - val_loss: 0.2465 - val_acc: 0.9245\n",
      "Epoch 97/100\n",
      "2499/2499 [==============================] - 0s 61us/step - loss: 0.1745 - acc: 0.9464 - val_loss: 0.2477 - val_acc: 0.9257\n",
      "Epoch 98/100\n",
      "2499/2499 [==============================] - 0s 65us/step - loss: 0.1745 - acc: 0.9452 - val_loss: 0.2467 - val_acc: 0.9257\n",
      "Epoch 99/100\n",
      "2499/2499 [==============================] - 0s 67us/step - loss: 0.1740 - acc: 0.9444 - val_loss: 0.2467 - val_acc: 0.9257\n",
      "Epoch 100/100\n",
      "2499/2499 [==============================] - 0s 73us/step - loss: 0.1737 - acc: 0.9460 - val_loss: 0.2448 - val_acc: 0.9293\n"
     ]
    }
   ],
   "source": [
    "# Fit it\n",
    "history=model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=100,batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Look, Ma, the machine is learning!\n",
    "train_loss=history.history['loss']\n",
    "test_loss=history.history['val_loss']\n",
    "plt.plot(train_loss,label='Training loss')\n",
    "plt.plot(test_loss,label='Test loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without regularization, **val_loss: **."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='l1'></a>\n",
    "## Regularization Method 1: L1 and L2 \n",
    "---\n",
    "Just as we did with linear and logistic regression, we can use `L1` and `L2` regularization on our neural networks.\n",
    "\n",
    "Neural networks are just large combinations of linear functions that are modified using some activation function:\n",
    "\n",
    "$$z = b_0 + \\sum_{j=1}^p w_j x_j$$\n",
    "$$a = g(z)$$\n",
    "\n",
    "Where $x_j$ is one input (i.e. one observation's blood pressure, one observation's sex, etc.), $w_j$ is the weight/coefficient for that particular variable, $b_0$ is our bias, and $g$ is our activation function. If we used a sigmoid function as we would for logistic regression, $g$ would be:\n",
    "$$g(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "After we've done this for every node, we can then compute the loss for every node as a function of their parameters:\n",
    "$$\\text{loss} = \\frac{1}{m}\\sum_{i=1}^{m}L(\\hat{y}_i, y_i)$$\n",
    "\n",
    "This is our average loss. In a regression context, this is usually mean squared error; in a classification context this might be categorical cross-entropy or something else. This would be our loss function *without regularization*.\n",
    "\n",
    "We'll then implement gradient descent:\n",
    "\n",
    "$$w_j := w_j -\\alpha\\frac{\\partial \\text{loss}}{\\partial w_j}$$\n",
    "\n",
    "where $\\alpha$ is our learning rate and $\\frac{\\partial \\text{loss}}{\\partial w_j}$ represents the partial derivative of our loss function with respect to our weight $w_j$.\n",
    "\n",
    "This is how we implement gradient descent **without regularization**.\n",
    "\n",
    "#### So, how do we implement gradient descent with `L1` or `L2` regularization?\n",
    "\n",
    "> We just change the loss function to add a penalty! If we want to add a penalty term, we do the **exact same thing** we did with linear or logistic regression:\n",
    "\n",
    "$$\\text{L2 regularized loss} = \\frac{1}{m}\\sum_{i=1}^{m}L(\\hat{y}_i, y_i) + \\frac{\\lambda}{2m}\\sum_{l=1}^{L}||w_{[l]}||^2$$\n",
    "\n",
    "Now, $$\\frac{\\partial \\text{L2 regularized loss}}{\\partial w_{[l]}} = \\frac{\\partial \\text{loss}}{\\partial w_j} + \\frac{\\lambda}{m}w_j$$\n",
    "and\n",
    "$$w_j := w_j -\\alpha\\frac{\\partial \\text{L2 regularized loss}}{\\partial w_j}$$\n",
    "\n",
    "In this example we used `L2` regularization, although `L1` works in the same way. You may see `L2` regularization referred to as \"*weight decay*.\"\n",
    "\n",
    "**Practical Note:** According to Andrew Ng, `L2` (as opposed to `L1`) is generally used for regularizing neural networks and it's rare to find `L1`.\n",
    "\n",
    "As before, $\\lambda$ is a hyperparameter to be selected by constructing multiple networks and identifying which value performs the best.\n",
    "- Intuitively, as $\\lambda \\rightarrow \\infty$, our weights get closer and closer to 0 (just like when we regularized our linear models before). But gets close to high bias\n",
    "- Intuitively, as $\\lambda \\rightarrow \\infty$, if we're using the `sigmoid` or `tanh` activation functions, we force our weights to stay in that \"linear\" region in the middle. This speeds up the learning process!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2499 samples, validate on 834 samples\n",
      "Epoch 1/100\n",
      "2499/2499 [==============================] - 0s 188us/step - loss: 0.6577 - acc: 0.8527 - val_loss: 0.6009 - val_acc: 0.8609\n",
      "Epoch 2/100\n",
      "2499/2499 [==============================] - 0s 64us/step - loss: 0.5719 - acc: 0.8531 - val_loss: 0.5394 - val_acc: 0.8609\n",
      "Epoch 3/100\n",
      "2499/2499 [==============================] - 0s 66us/step - loss: 0.5157 - acc: 0.8535 - val_loss: 0.4962 - val_acc: 0.8609\n",
      "Epoch 4/100\n",
      "2499/2499 [==============================] - 0s 65us/step - loss: 0.4751 - acc: 0.8535 - val_loss: 0.4636 - val_acc: 0.8609\n",
      "Epoch 5/100\n",
      "2499/2499 [==============================] - 0s 66us/step - loss: 0.4456 - acc: 0.8547 - val_loss: 0.4390 - val_acc: 0.8621\n",
      "Epoch 6/100\n",
      "2499/2499 [==============================] - 0s 62us/step - loss: 0.4234 - acc: 0.8563 - val_loss: 0.4222 - val_acc: 0.8621\n",
      "Epoch 7/100\n",
      "2499/2499 [==============================] - 0s 64us/step - loss: 0.4073 - acc: 0.8575 - val_loss: 0.4068 - val_acc: 0.8645\n",
      "Epoch 8/100\n",
      "2499/2499 [==============================] - 0s 67us/step - loss: 0.3953 - acc: 0.8627 - val_loss: 0.3961 - val_acc: 0.8645\n",
      "Epoch 9/100\n",
      "2499/2499 [==============================] - 0s 65us/step - loss: 0.3849 - acc: 0.8623 - val_loss: 0.3871 - val_acc: 0.8657\n",
      "Epoch 10/100\n",
      "2499/2499 [==============================] - 0s 65us/step - loss: 0.3766 - acc: 0.8655 - val_loss: 0.3801 - val_acc: 0.8681\n",
      "Epoch 11/100\n",
      "2499/2499 [==============================] - 0s 64us/step - loss: 0.3691 - acc: 0.8691 - val_loss: 0.3734 - val_acc: 0.8669\n",
      "Epoch 12/100\n",
      "2499/2499 [==============================] - 0s 66us/step - loss: 0.3628 - acc: 0.8711 - val_loss: 0.3686 - val_acc: 0.8717\n",
      "Epoch 13/100\n",
      "2499/2499 [==============================] - 0s 68us/step - loss: 0.3568 - acc: 0.8764 - val_loss: 0.3643 - val_acc: 0.8753\n",
      "Epoch 14/100\n",
      "2499/2499 [==============================] - 0s 67us/step - loss: 0.3511 - acc: 0.8804 - val_loss: 0.3589 - val_acc: 0.8741\n",
      "Epoch 15/100\n",
      "2499/2499 [==============================] - 0s 64us/step - loss: 0.3466 - acc: 0.8788 - val_loss: 0.3557 - val_acc: 0.8801\n",
      "Epoch 16/100\n",
      "2499/2499 [==============================] - 0s 69us/step - loss: 0.3419 - acc: 0.8872 - val_loss: 0.3524 - val_acc: 0.8837\n",
      "Epoch 17/100\n",
      "2499/2499 [==============================] - 0s 66us/step - loss: 0.3370 - acc: 0.8860 - val_loss: 0.3502 - val_acc: 0.8837\n",
      "Epoch 18/100\n",
      "2499/2499 [==============================] - 0s 86us/step - loss: 0.3334 - acc: 0.8920 - val_loss: 0.3463 - val_acc: 0.8861\n",
      "Epoch 19/100\n",
      "2499/2499 [==============================] - 0s 78us/step - loss: 0.3301 - acc: 0.8936 - val_loss: 0.3439 - val_acc: 0.8873\n",
      "Epoch 20/100\n",
      "2499/2499 [==============================] - 0s 91us/step - loss: 0.3277 - acc: 0.8948 - val_loss: 0.3426 - val_acc: 0.8861\n",
      "Epoch 21/100\n",
      "2499/2499 [==============================] - 0s 68us/step - loss: 0.3244 - acc: 0.8972 - val_loss: 0.3404 - val_acc: 0.8933\n",
      "Epoch 22/100\n",
      "2499/2499 [==============================] - 0s 66us/step - loss: 0.3223 - acc: 0.8988 - val_loss: 0.3390 - val_acc: 0.8909\n",
      "Epoch 23/100\n",
      "2499/2499 [==============================] - 0s 79us/step - loss: 0.3209 - acc: 0.8968 - val_loss: 0.3388 - val_acc: 0.8945\n",
      "Epoch 24/100\n",
      "2499/2499 [==============================] - 0s 66us/step - loss: 0.3195 - acc: 0.9016 - val_loss: 0.3366 - val_acc: 0.8957\n",
      "Epoch 25/100\n",
      "2499/2499 [==============================] - 0s 67us/step - loss: 0.3185 - acc: 0.9028 - val_loss: 0.3348 - val_acc: 0.8873\n",
      "Epoch 26/100\n",
      "2499/2499 [==============================] - 0s 64us/step - loss: 0.3180 - acc: 0.8960 - val_loss: 0.3333 - val_acc: 0.8945\n",
      "Epoch 27/100\n",
      "2499/2499 [==============================] - 0s 67us/step - loss: 0.3166 - acc: 0.9032 - val_loss: 0.3348 - val_acc: 0.8945\n",
      "Epoch 28/100\n",
      "2499/2499 [==============================] - 0s 65us/step - loss: 0.3158 - acc: 0.9004 - val_loss: 0.3336 - val_acc: 0.8921\n",
      "Epoch 29/100\n",
      "2499/2499 [==============================] - 0s 66us/step - loss: 0.3146 - acc: 0.9064 - val_loss: 0.3335 - val_acc: 0.8933\n",
      "Epoch 30/100\n",
      "2499/2499 [==============================] - 0s 71us/step - loss: 0.3143 - acc: 0.9024 - val_loss: 0.3327 - val_acc: 0.8981\n",
      "Epoch 31/100\n",
      "2499/2499 [==============================] - 0s 63us/step - loss: 0.3134 - acc: 0.9060 - val_loss: 0.3322 - val_acc: 0.8969\n",
      "Epoch 32/100\n",
      "2499/2499 [==============================] - 0s 67us/step - loss: 0.3124 - acc: 0.9064 - val_loss: 0.3320 - val_acc: 0.8957\n",
      "Epoch 33/100\n",
      "2499/2499 [==============================] - 0s 67us/step - loss: 0.3127 - acc: 0.9080 - val_loss: 0.3319 - val_acc: 0.8957\n",
      "Epoch 34/100\n",
      "2499/2499 [==============================] - 0s 67us/step - loss: 0.3118 - acc: 0.9072 - val_loss: 0.3310 - val_acc: 0.8945\n",
      "Epoch 35/100\n",
      "2499/2499 [==============================] - 0s 66us/step - loss: 0.3116 - acc: 0.9064 - val_loss: 0.3310 - val_acc: 0.8969\n",
      "Epoch 36/100\n",
      "2499/2499 [==============================] - 0s 66us/step - loss: 0.3109 - acc: 0.9092 - val_loss: 0.3302 - val_acc: 0.8945\n",
      "Epoch 37/100\n",
      "2499/2499 [==============================] - 0s 63us/step - loss: 0.3105 - acc: 0.9068 - val_loss: 0.3302 - val_acc: 0.8969\n",
      "Epoch 38/100\n",
      "2499/2499 [==============================] - 0s 78us/step - loss: 0.3103 - acc: 0.9056 - val_loss: 0.3307 - val_acc: 0.8993\n",
      "Epoch 39/100\n",
      "2499/2499 [==============================] - 0s 63us/step - loss: 0.3099 - acc: 0.9092 - val_loss: 0.3306 - val_acc: 0.9017\n",
      "Epoch 40/100\n",
      "2499/2499 [==============================] - 0s 65us/step - loss: 0.3100 - acc: 0.9060 - val_loss: 0.3292 - val_acc: 0.8969\n",
      "Epoch 41/100\n",
      "2499/2499 [==============================] - 0s 63us/step - loss: 0.3092 - acc: 0.9092 - val_loss: 0.3298 - val_acc: 0.8957\n",
      "Epoch 42/100\n",
      "2499/2499 [==============================] - 0s 63us/step - loss: 0.3095 - acc: 0.9056 - val_loss: 0.3286 - val_acc: 0.8981\n",
      "Epoch 43/100\n",
      "2499/2499 [==============================] - 0s 61us/step - loss: 0.3087 - acc: 0.9092 - val_loss: 0.3288 - val_acc: 0.8993\n",
      "Epoch 44/100\n",
      "2499/2499 [==============================] - 0s 62us/step - loss: 0.3087 - acc: 0.9108 - val_loss: 0.3287 - val_acc: 0.8969\n",
      "Epoch 45/100\n",
      "2499/2499 [==============================] - 0s 66us/step - loss: 0.3087 - acc: 0.9088 - val_loss: 0.3283 - val_acc: 0.9029\n",
      "Epoch 46/100\n",
      "2499/2499 [==============================] - 0s 68us/step - loss: 0.3083 - acc: 0.9092 - val_loss: 0.3289 - val_acc: 0.8993\n",
      "Epoch 47/100\n",
      "2499/2499 [==============================] - 0s 68us/step - loss: 0.3080 - acc: 0.9080 - val_loss: 0.3286 - val_acc: 0.8993\n",
      "Epoch 48/100\n",
      "2499/2499 [==============================] - 0s 66us/step - loss: 0.3081 - acc: 0.9096 - val_loss: 0.3284 - val_acc: 0.8981\n",
      "Epoch 49/100\n",
      "2499/2499 [==============================] - 0s 73us/step - loss: 0.3079 - acc: 0.9060 - val_loss: 0.3279 - val_acc: 0.9017\n",
      "Epoch 50/100\n",
      "2499/2499 [==============================] - 0s 69us/step - loss: 0.3076 - acc: 0.9116 - val_loss: 0.3286 - val_acc: 0.9017\n",
      "Epoch 51/100\n",
      "2499/2499 [==============================] - 0s 65us/step - loss: 0.3074 - acc: 0.9100 - val_loss: 0.3283 - val_acc: 0.9005\n",
      "Epoch 52/100\n",
      "2499/2499 [==============================] - 0s 68us/step - loss: 0.3073 - acc: 0.9100 - val_loss: 0.3273 - val_acc: 0.8981\n",
      "Epoch 53/100\n",
      "2499/2499 [==============================] - 0s 65us/step - loss: 0.3072 - acc: 0.9096 - val_loss: 0.3275 - val_acc: 0.8969\n",
      "Epoch 54/100\n",
      "2499/2499 [==============================] - 0s 72us/step - loss: 0.3069 - acc: 0.9084 - val_loss: 0.3277 - val_acc: 0.9005\n",
      "Epoch 55/100\n",
      "2499/2499 [==============================] - 0s 65us/step - loss: 0.3072 - acc: 0.9068 - val_loss: 0.3280 - val_acc: 0.9053\n",
      "Epoch 56/100\n",
      "2499/2499 [==============================] - 0s 66us/step - loss: 0.3069 - acc: 0.9104 - val_loss: 0.3279 - val_acc: 0.9005\n",
      "Epoch 57/100\n",
      "2499/2499 [==============================] - 0s 77us/step - loss: 0.3069 - acc: 0.9116 - val_loss: 0.3268 - val_acc: 0.9041\n",
      "Epoch 58/100\n",
      "2499/2499 [==============================] - 0s 79us/step - loss: 0.3069 - acc: 0.9084 - val_loss: 0.3273 - val_acc: 0.9017\n",
      "Epoch 59/100\n",
      "2499/2499 [==============================] - 0s 72us/step - loss: 0.3072 - acc: 0.9064 - val_loss: 0.3267 - val_acc: 0.8993\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2499/2499 [==============================] - 0s 73us/step - loss: 0.3073 - acc: 0.9088 - val_loss: 0.3287 - val_acc: 0.9029\n",
      "Epoch 61/100\n",
      "2499/2499 [==============================] - 0s 83us/step - loss: 0.3069 - acc: 0.9096 - val_loss: 0.3280 - val_acc: 0.9053\n",
      "Epoch 62/100\n",
      "2499/2499 [==============================] - 0s 76us/step - loss: 0.3064 - acc: 0.9068 - val_loss: 0.3282 - val_acc: 0.9017\n",
      "Epoch 63/100\n",
      "2499/2499 [==============================] - 0s 65us/step - loss: 0.3068 - acc: 0.9092 - val_loss: 0.3273 - val_acc: 0.9017\n",
      "Epoch 64/100\n",
      "2499/2499 [==============================] - 0s 65us/step - loss: 0.3064 - acc: 0.9108 - val_loss: 0.3269 - val_acc: 0.8969\n",
      "Epoch 65/100\n",
      "2499/2499 [==============================] - 0s 66us/step - loss: 0.3074 - acc: 0.9108 - val_loss: 0.3264 - val_acc: 0.9005\n",
      "Epoch 66/100\n",
      "2499/2499 [==============================] - 0s 81us/step - loss: 0.3066 - acc: 0.9064 - val_loss: 0.3275 - val_acc: 0.9017\n",
      "Epoch 67/100\n",
      "2499/2499 [==============================] - 0s 85us/step - loss: 0.3061 - acc: 0.9080 - val_loss: 0.3263 - val_acc: 0.8981\n",
      "Epoch 68/100\n",
      "2499/2499 [==============================] - 0s 71us/step - loss: 0.3060 - acc: 0.9092 - val_loss: 0.3274 - val_acc: 0.9017\n",
      "Epoch 69/100\n",
      "2499/2499 [==============================] - 0s 64us/step - loss: 0.3060 - acc: 0.9088 - val_loss: 0.3268 - val_acc: 0.9017\n",
      "Epoch 70/100\n",
      "2499/2499 [==============================] - 0s 62us/step - loss: 0.3060 - acc: 0.9092 - val_loss: 0.3269 - val_acc: 0.9041\n",
      "Epoch 71/100\n",
      "2499/2499 [==============================] - 0s 75us/step - loss: 0.3060 - acc: 0.9124 - val_loss: 0.3268 - val_acc: 0.8993\n",
      "Epoch 72/100\n",
      "2499/2499 [==============================] - 0s 70us/step - loss: 0.3064 - acc: 0.9072 - val_loss: 0.3267 - val_acc: 0.9017\n",
      "Epoch 73/100\n",
      "2499/2499 [==============================] - 0s 74us/step - loss: 0.3059 - acc: 0.9112 - val_loss: 0.3279 - val_acc: 0.9041\n",
      "Epoch 74/100\n",
      "2499/2499 [==============================] - 0s 75us/step - loss: 0.3076 - acc: 0.9136 - val_loss: 0.3288 - val_acc: 0.9029\n",
      "Epoch 75/100\n",
      "2499/2499 [==============================] - 0s 74us/step - loss: 0.3065 - acc: 0.9132 - val_loss: 0.3275 - val_acc: 0.9005\n",
      "Epoch 76/100\n",
      "2499/2499 [==============================] - 0s 76us/step - loss: 0.3059 - acc: 0.9096 - val_loss: 0.3267 - val_acc: 0.8993\n",
      "Epoch 77/100\n",
      "2499/2499 [==============================] - 0s 71us/step - loss: 0.3055 - acc: 0.9092 - val_loss: 0.3269 - val_acc: 0.9017\n",
      "Epoch 78/100\n",
      "2499/2499 [==============================] - 0s 69us/step - loss: 0.3061 - acc: 0.9108 - val_loss: 0.3265 - val_acc: 0.9017\n",
      "Epoch 79/100\n",
      "2499/2499 [==============================] - 0s 78us/step - loss: 0.3056 - acc: 0.9104 - val_loss: 0.3268 - val_acc: 0.8981\n",
      "Epoch 80/100\n",
      "2499/2499 [==============================] - 0s 76us/step - loss: 0.3063 - acc: 0.9116 - val_loss: 0.3277 - val_acc: 0.8981\n",
      "Epoch 81/100\n",
      "2499/2499 [==============================] - 0s 75us/step - loss: 0.3059 - acc: 0.9132 - val_loss: 0.3279 - val_acc: 0.8969\n",
      "Epoch 82/100\n",
      "2499/2499 [==============================] - 0s 76us/step - loss: 0.3063 - acc: 0.9080 - val_loss: 0.3271 - val_acc: 0.9041\n",
      "Epoch 83/100\n",
      "2499/2499 [==============================] - 0s 76us/step - loss: 0.3060 - acc: 0.9124 - val_loss: 0.3274 - val_acc: 0.9017\n",
      "Epoch 84/100\n",
      "2499/2499 [==============================] - 0s 73us/step - loss: 0.3064 - acc: 0.9144 - val_loss: 0.3275 - val_acc: 0.9017\n",
      "Epoch 85/100\n",
      "2499/2499 [==============================] - 0s 78us/step - loss: 0.3056 - acc: 0.9100 - val_loss: 0.3267 - val_acc: 0.9005\n",
      "Epoch 86/100\n",
      "2499/2499 [==============================] - 0s 80us/step - loss: 0.3055 - acc: 0.9092 - val_loss: 0.3262 - val_acc: 0.8993\n",
      "Epoch 87/100\n",
      "2499/2499 [==============================] - 0s 77us/step - loss: 0.3058 - acc: 0.9104 - val_loss: 0.3271 - val_acc: 0.9029\n",
      "Epoch 88/100\n",
      "2499/2499 [==============================] - 0s 70us/step - loss: 0.3054 - acc: 0.9112 - val_loss: 0.3271 - val_acc: 0.9029\n",
      "Epoch 89/100\n",
      "2499/2499 [==============================] - 0s 89us/step - loss: 0.3054 - acc: 0.9120 - val_loss: 0.3277 - val_acc: 0.9017\n",
      "Epoch 90/100\n",
      "2499/2499 [==============================] - 0s 81us/step - loss: 0.3059 - acc: 0.9112 - val_loss: 0.3267 - val_acc: 0.9017\n",
      "Epoch 91/100\n",
      "2499/2499 [==============================] - 0s 72us/step - loss: 0.3058 - acc: 0.9088 - val_loss: 0.3268 - val_acc: 0.8993\n",
      "Epoch 92/100\n",
      "2499/2499 [==============================] - 0s 76us/step - loss: 0.3054 - acc: 0.9124 - val_loss: 0.3267 - val_acc: 0.9005\n",
      "Epoch 93/100\n",
      "2499/2499 [==============================] - 0s 68us/step - loss: 0.3053 - acc: 0.9080 - val_loss: 0.3270 - val_acc: 0.9005\n",
      "Epoch 94/100\n",
      "2499/2499 [==============================] - 0s 73us/step - loss: 0.3053 - acc: 0.9108 - val_loss: 0.3266 - val_acc: 0.9029\n",
      "Epoch 95/100\n",
      "2499/2499 [==============================] - 0s 72us/step - loss: 0.3055 - acc: 0.9096 - val_loss: 0.3266 - val_acc: 0.9041\n",
      "Epoch 96/100\n",
      "2499/2499 [==============================] - 0s 66us/step - loss: 0.3052 - acc: 0.9128 - val_loss: 0.3268 - val_acc: 0.9005\n",
      "Epoch 97/100\n",
      "2499/2499 [==============================] - 0s 64us/step - loss: 0.3055 - acc: 0.9132 - val_loss: 0.3255 - val_acc: 0.9017\n",
      "Epoch 98/100\n",
      "2499/2499 [==============================] - 0s 106us/step - loss: 0.3056 - acc: 0.9084 - val_loss: 0.3270 - val_acc: 0.9029\n",
      "Epoch 99/100\n",
      "2499/2499 [==============================] - 0s 77us/step - loss: 0.3054 - acc: 0.9108 - val_loss: 0.3272 - val_acc: 0.9029\n",
      "Epoch 100/100\n",
      "2499/2499 [==============================] - 0s 80us/step - loss: 0.3053 - acc: 0.9116 - val_loss: 0.3267 - val_acc: 0.9041\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "model_2 = Sequential()\n",
    "\n",
    "n_input = X_train.shape[1]\n",
    "n_hidden = n_input\n",
    "n_output = 1\n",
    "\n",
    "model_2.add(Dense(n_hidden, input_dim=n_input, activation='relu',\n",
    "                  kernel_regularizer=regularizers.l2(0.01)))\n",
    "model_2.add(Dense(n_output, activation='sigmoid', \n",
    "                  kernel_regularizer=regularizers.l2(0.01)))\n",
    "\n",
    "model_2.compile(loss='binary_crossentropy', optimizer='adam', \n",
    "                metrics=['acc'])\n",
    "\n",
    "history=model_2.fit(X_train, y_train, validation_data=(X_test, y_test), \n",
    "            epochs=100, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss=history.history['loss']\n",
    "test_loss=history.history['val_loss']\n",
    "plt.plot(train_loss,label='Training loss')\n",
    "plt.plot(test_loss,label='Test loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2499 samples, validate on 834 samples\n",
      "Epoch 1/100\n",
      "2499/2499 [==============================] - 1s 218us/step - loss: 1.7296 - acc: 0.4030 - val_loss: 1.4374 - val_acc: 0.5084\n",
      "Epoch 2/100\n",
      "2499/2499 [==============================] - 0s 75us/step - loss: 1.2614 - acc: 0.6439 - val_loss: 1.1038 - val_acc: 0.7626\n",
      "Epoch 3/100\n",
      "2499/2499 [==============================] - 0s 73us/step - loss: 1.0044 - acc: 0.8131 - val_loss: 0.8944 - val_acc: 0.8621\n",
      "Epoch 4/100\n",
      "2499/2499 [==============================] - 0s 76us/step - loss: 0.8275 - acc: 0.8523 - val_loss: 0.7449 - val_acc: 0.8645\n",
      "Epoch 5/100\n",
      "2499/2499 [==============================] - 0s 78us/step - loss: 0.6975 - acc: 0.8587 - val_loss: 0.6373 - val_acc: 0.8633\n",
      "Epoch 6/100\n",
      "2499/2499 [==============================] - 0s 85us/step - loss: 0.6049 - acc: 0.8571 - val_loss: 0.5648 - val_acc: 0.8621\n",
      "Epoch 7/100\n",
      "2499/2499 [==============================] - 0s 77us/step - loss: 0.5438 - acc: 0.8567 - val_loss: 0.5168 - val_acc: 0.8609\n",
      "Epoch 8/100\n",
      "2499/2499 [==============================] - 0s 101us/step - loss: 0.5052 - acc: 0.8547 - val_loss: 0.4871 - val_acc: 0.8609\n",
      "Epoch 9/100\n",
      "2499/2499 [==============================] - 0s 86us/step - loss: 0.4804 - acc: 0.8535 - val_loss: 0.4683 - val_acc: 0.8609\n",
      "Epoch 10/100\n",
      "2499/2499 [==============================] - 0s 87us/step - loss: 0.4630 - acc: 0.8531 - val_loss: 0.4551 - val_acc: 0.8609\n",
      "Epoch 11/100\n",
      "2499/2499 [==============================] - 0s 85us/step - loss: 0.4527 - acc: 0.8531 - val_loss: 0.4477 - val_acc: 0.8609\n",
      "Epoch 12/100\n",
      "2499/2499 [==============================] - 0s 88us/step - loss: 0.4449 - acc: 0.8531 - val_loss: 0.4403 - val_acc: 0.8609\n",
      "Epoch 13/100\n",
      "2499/2499 [==============================] - 0s 85us/step - loss: 0.4391 - acc: 0.8531 - val_loss: 0.4361 - val_acc: 0.8609\n",
      "Epoch 14/100\n",
      "2499/2499 [==============================] - 0s 97us/step - loss: 0.4356 - acc: 0.8531 - val_loss: 0.4330 - val_acc: 0.8609\n",
      "Epoch 15/100\n",
      "2499/2499 [==============================] - 0s 87us/step - loss: 0.4332 - acc: 0.8531 - val_loss: 0.4308 - val_acc: 0.8609\n",
      "Epoch 16/100\n",
      "2499/2499 [==============================] - 0s 84us/step - loss: 0.4309 - acc: 0.8531 - val_loss: 0.4283 - val_acc: 0.8609\n",
      "Epoch 17/100\n",
      "2499/2499 [==============================] - 0s 78us/step - loss: 0.4304 - acc: 0.8531 - val_loss: 0.4257 - val_acc: 0.8609\n",
      "Epoch 18/100\n",
      "2499/2499 [==============================] - 0s 79us/step - loss: 0.4279 - acc: 0.8531 - val_loss: 0.4244 - val_acc: 0.8609\n",
      "Epoch 19/100\n",
      "2499/2499 [==============================] - 0s 77us/step - loss: 0.4263 - acc: 0.8531 - val_loss: 0.4231 - val_acc: 0.8609\n",
      "Epoch 20/100\n",
      "2499/2499 [==============================] - 0s 75us/step - loss: 0.4249 - acc: 0.8531 - val_loss: 0.4213 - val_acc: 0.8609\n",
      "Epoch 21/100\n",
      "2499/2499 [==============================] - 0s 80us/step - loss: 0.4232 - acc: 0.8531 - val_loss: 0.4198 - val_acc: 0.8609\n",
      "Epoch 22/100\n",
      "2499/2499 [==============================] - 0s 79us/step - loss: 0.4218 - acc: 0.8531 - val_loss: 0.4184 - val_acc: 0.8609\n",
      "Epoch 23/100\n",
      "2499/2499 [==============================] - 0s 77us/step - loss: 0.4207 - acc: 0.8531 - val_loss: 0.4175 - val_acc: 0.8609\n",
      "Epoch 24/100\n",
      "2499/2499 [==============================] - 0s 75us/step - loss: 0.4198 - acc: 0.8531 - val_loss: 0.4168 - val_acc: 0.8609\n",
      "Epoch 25/100\n",
      "2499/2499 [==============================] - 0s 76us/step - loss: 0.4190 - acc: 0.8531 - val_loss: 0.4158 - val_acc: 0.8609\n",
      "Epoch 26/100\n",
      "2499/2499 [==============================] - 0s 69us/step - loss: 0.4180 - acc: 0.8531 - val_loss: 0.4146 - val_acc: 0.8609\n",
      "Epoch 27/100\n",
      "2499/2499 [==============================] - 0s 79us/step - loss: 0.4171 - acc: 0.8531 - val_loss: 0.4138 - val_acc: 0.8609\n",
      "Epoch 28/100\n",
      "2499/2499 [==============================] - 0s 75us/step - loss: 0.4166 - acc: 0.8531 - val_loss: 0.4127 - val_acc: 0.8609\n",
      "Epoch 29/100\n",
      "2499/2499 [==============================] - 0s 80us/step - loss: 0.4154 - acc: 0.8531 - val_loss: 0.4117 - val_acc: 0.8609\n",
      "Epoch 30/100\n",
      "2499/2499 [==============================] - 0s 81us/step - loss: 0.4142 - acc: 0.8531 - val_loss: 0.4106 - val_acc: 0.8609\n",
      "Epoch 31/100\n",
      "2499/2499 [==============================] - 0s 77us/step - loss: 0.4134 - acc: 0.8531 - val_loss: 0.4096 - val_acc: 0.8609\n",
      "Epoch 32/100\n",
      "2499/2499 [==============================] - 0s 80us/step - loss: 0.4125 - acc: 0.8531 - val_loss: 0.4091 - val_acc: 0.8609\n",
      "Epoch 33/100\n",
      "2499/2499 [==============================] - 0s 81us/step - loss: 0.4118 - acc: 0.8531 - val_loss: 0.4084 - val_acc: 0.8609\n",
      "Epoch 34/100\n",
      "2499/2499 [==============================] - 0s 81us/step - loss: 0.4113 - acc: 0.8531 - val_loss: 0.4079 - val_acc: 0.8609\n",
      "Epoch 35/100\n",
      "2499/2499 [==============================] - 0s 82us/step - loss: 0.4109 - acc: 0.8531 - val_loss: 0.4074 - val_acc: 0.8609\n",
      "Epoch 36/100\n",
      "2499/2499 [==============================] - 0s 82us/step - loss: 0.4102 - acc: 0.8531 - val_loss: 0.4067 - val_acc: 0.8609\n",
      "Epoch 37/100\n",
      "2499/2499 [==============================] - 0s 84us/step - loss: 0.4102 - acc: 0.8531 - val_loss: 0.4067 - val_acc: 0.8609\n",
      "Epoch 38/100\n",
      "2499/2499 [==============================] - 0s 90us/step - loss: 0.4095 - acc: 0.8531 - val_loss: 0.4058 - val_acc: 0.8609\n",
      "Epoch 39/100\n",
      "2499/2499 [==============================] - 0s 95us/step - loss: 0.4089 - acc: 0.8531 - val_loss: 0.4054 - val_acc: 0.8609\n",
      "Epoch 40/100\n",
      "2499/2499 [==============================] - 0s 82us/step - loss: 0.4086 - acc: 0.8531 - val_loss: 0.4054 - val_acc: 0.8609\n",
      "Epoch 41/100\n",
      "2499/2499 [==============================] - 0s 89us/step - loss: 0.4083 - acc: 0.8531 - val_loss: 0.4054 - val_acc: 0.8609\n",
      "Epoch 42/100\n",
      "2499/2499 [==============================] - 0s 81us/step - loss: 0.4078 - acc: 0.8531 - val_loss: 0.4049 - val_acc: 0.8609\n",
      "Epoch 43/100\n",
      "2499/2499 [==============================] - 0s 95us/step - loss: 0.4075 - acc: 0.8531 - val_loss: 0.4042 - val_acc: 0.8609\n",
      "Epoch 44/100\n",
      "2499/2499 [==============================] - 0s 105us/step - loss: 0.4068 - acc: 0.8531 - val_loss: 0.4038 - val_acc: 0.8609\n",
      "Epoch 45/100\n",
      "2499/2499 [==============================] - 0s 87us/step - loss: 0.4063 - acc: 0.8531 - val_loss: 0.4028 - val_acc: 0.8609\n",
      "Epoch 46/100\n",
      "2499/2499 [==============================] - 0s 75us/step - loss: 0.4056 - acc: 0.8531 - val_loss: 0.4023 - val_acc: 0.8609\n",
      "Epoch 47/100\n",
      "2499/2499 [==============================] - 0s 88us/step - loss: 0.4050 - acc: 0.8531 - val_loss: 0.4022 - val_acc: 0.8609\n",
      "Epoch 48/100\n",
      "2499/2499 [==============================] - 0s 85us/step - loss: 0.4047 - acc: 0.8531 - val_loss: 0.4018 - val_acc: 0.8609\n",
      "Epoch 49/100\n",
      "2499/2499 [==============================] - 0s 91us/step - loss: 0.4036 - acc: 0.8531 - val_loss: 0.4013 - val_acc: 0.8609\n",
      "Epoch 50/100\n",
      "2499/2499 [==============================] - 0s 85us/step - loss: 0.4037 - acc: 0.8531 - val_loss: 0.4009 - val_acc: 0.8609\n",
      "Epoch 51/100\n",
      "2499/2499 [==============================] - 0s 83us/step - loss: 0.4029 - acc: 0.8531 - val_loss: 0.4011 - val_acc: 0.8609\n",
      "Epoch 52/100\n",
      "2499/2499 [==============================] - 0s 89us/step - loss: 0.4026 - acc: 0.8531 - val_loss: 0.4007 - val_acc: 0.8609\n",
      "Epoch 53/100\n",
      "2499/2499 [==============================] - 0s 78us/step - loss: 0.4026 - acc: 0.8531 - val_loss: 0.4004 - val_acc: 0.8609\n",
      "Epoch 54/100\n",
      "2499/2499 [==============================] - 0s 87us/step - loss: 0.4020 - acc: 0.8531 - val_loss: 0.4002 - val_acc: 0.8609\n",
      "Epoch 55/100\n",
      "2499/2499 [==============================] - 0s 84us/step - loss: 0.4019 - acc: 0.8531 - val_loss: 0.3996 - val_acc: 0.8609\n",
      "Epoch 56/100\n",
      "2499/2499 [==============================] - 0s 85us/step - loss: 0.4018 - acc: 0.8531 - val_loss: 0.3994 - val_acc: 0.8609\n",
      "Epoch 57/100\n",
      "2499/2499 [==============================] - 0s 80us/step - loss: 0.4015 - acc: 0.8531 - val_loss: 0.3994 - val_acc: 0.8609\n",
      "Epoch 58/100\n",
      "2499/2499 [==============================] - 0s 69us/step - loss: 0.4012 - acc: 0.8531 - val_loss: 0.3991 - val_acc: 0.8609\n",
      "Epoch 59/100\n",
      "2499/2499 [==============================] - 0s 76us/step - loss: 0.4009 - acc: 0.8531 - val_loss: 0.3988 - val_acc: 0.8609\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2499/2499 [==============================] - 0s 86us/step - loss: 0.4005 - acc: 0.8531 - val_loss: 0.3984 - val_acc: 0.8609\n",
      "Epoch 61/100\n",
      "2499/2499 [==============================] - 0s 83us/step - loss: 0.3999 - acc: 0.8531 - val_loss: 0.3980 - val_acc: 0.8609\n",
      "Epoch 62/100\n",
      "2499/2499 [==============================] - 0s 78us/step - loss: 0.3995 - acc: 0.8531 - val_loss: 0.3973 - val_acc: 0.8609\n",
      "Epoch 63/100\n",
      "2499/2499 [==============================] - 0s 75us/step - loss: 0.3990 - acc: 0.8531 - val_loss: 0.3969 - val_acc: 0.8609\n",
      "Epoch 64/100\n",
      "2499/2499 [==============================] - 0s 67us/step - loss: 0.3989 - acc: 0.8531 - val_loss: 0.3964 - val_acc: 0.8609\n",
      "Epoch 65/100\n",
      "2499/2499 [==============================] - 0s 79us/step - loss: 0.3980 - acc: 0.8531 - val_loss: 0.3958 - val_acc: 0.8609\n",
      "Epoch 66/100\n",
      "2499/2499 [==============================] - 0s 72us/step - loss: 0.3973 - acc: 0.8531 - val_loss: 0.3955 - val_acc: 0.8609\n",
      "Epoch 67/100\n",
      "2499/2499 [==============================] - 0s 79us/step - loss: 0.3966 - acc: 0.8531 - val_loss: 0.3949 - val_acc: 0.8609\n",
      "Epoch 68/100\n",
      "2499/2499 [==============================] - 0s 76us/step - loss: 0.3961 - acc: 0.8531 - val_loss: 0.3941 - val_acc: 0.8609\n",
      "Epoch 69/100\n",
      "2499/2499 [==============================] - 0s 75us/step - loss: 0.3955 - acc: 0.8531 - val_loss: 0.3936 - val_acc: 0.8609\n",
      "Epoch 70/100\n",
      "2499/2499 [==============================] - 0s 73us/step - loss: 0.3952 - acc: 0.8531 - val_loss: 0.3933 - val_acc: 0.8609\n",
      "Epoch 71/100\n",
      "2499/2499 [==============================] - 0s 91us/step - loss: 0.3949 - acc: 0.8531 - val_loss: 0.3933 - val_acc: 0.8609\n",
      "Epoch 72/100\n",
      "2499/2499 [==============================] - 0s 92us/step - loss: 0.3942 - acc: 0.8531 - val_loss: 0.3924 - val_acc: 0.8609\n",
      "Epoch 73/100\n",
      "2499/2499 [==============================] - 0s 98us/step - loss: 0.3942 - acc: 0.8531 - val_loss: 0.3926 - val_acc: 0.8609\n",
      "Epoch 74/100\n",
      "2499/2499 [==============================] - 0s 91us/step - loss: 0.3941 - acc: 0.8531 - val_loss: 0.3923 - val_acc: 0.8609\n",
      "Epoch 75/100\n",
      "2499/2499 [==============================] - 0s 85us/step - loss: 0.3937 - acc: 0.8531 - val_loss: 0.3923 - val_acc: 0.8609\n",
      "Epoch 76/100\n",
      "2499/2499 [==============================] - 0s 73us/step - loss: 0.3936 - acc: 0.8531 - val_loss: 0.3922 - val_acc: 0.8609\n",
      "Epoch 77/100\n",
      "2499/2499 [==============================] - 0s 82us/step - loss: 0.3933 - acc: 0.8531 - val_loss: 0.3919 - val_acc: 0.8609\n",
      "Epoch 78/100\n",
      "2499/2499 [==============================] - 0s 90us/step - loss: 0.3930 - acc: 0.8531 - val_loss: 0.3916 - val_acc: 0.8609\n",
      "Epoch 79/100\n",
      "2499/2499 [==============================] - 0s 88us/step - loss: 0.3929 - acc: 0.8531 - val_loss: 0.3915 - val_acc: 0.8609\n",
      "Epoch 80/100\n",
      "2499/2499 [==============================] - 0s 85us/step - loss: 0.3927 - acc: 0.8531 - val_loss: 0.3912 - val_acc: 0.8609\n",
      "Epoch 81/100\n",
      "2499/2499 [==============================] - 0s 76us/step - loss: 0.3925 - acc: 0.8531 - val_loss: 0.3909 - val_acc: 0.8609\n",
      "Epoch 82/100\n",
      "2499/2499 [==============================] - 0s 79us/step - loss: 0.3924 - acc: 0.8531 - val_loss: 0.3906 - val_acc: 0.8609\n",
      "Epoch 83/100\n",
      "2499/2499 [==============================] - 0s 82us/step - loss: 0.3919 - acc: 0.8531 - val_loss: 0.3901 - val_acc: 0.8609\n",
      "Epoch 84/100\n",
      "2499/2499 [==============================] - 0s 82us/step - loss: 0.3915 - acc: 0.8531 - val_loss: 0.3901 - val_acc: 0.8609\n",
      "Epoch 85/100\n",
      "2499/2499 [==============================] - 0s 83us/step - loss: 0.3913 - acc: 0.8531 - val_loss: 0.3895 - val_acc: 0.8609\n",
      "Epoch 86/100\n",
      "2499/2499 [==============================] - 0s 81us/step - loss: 0.3909 - acc: 0.8531 - val_loss: 0.3893 - val_acc: 0.8609\n",
      "Epoch 87/100\n",
      "2499/2499 [==============================] - 0s 75us/step - loss: 0.3905 - acc: 0.8531 - val_loss: 0.3894 - val_acc: 0.8609\n",
      "Epoch 88/100\n",
      "2499/2499 [==============================] - 0s 72us/step - loss: 0.3901 - acc: 0.8531 - val_loss: 0.3886 - val_acc: 0.8609\n",
      "Epoch 89/100\n",
      "2499/2499 [==============================] - 0s 86us/step - loss: 0.3895 - acc: 0.8531 - val_loss: 0.3882 - val_acc: 0.8609\n",
      "Epoch 90/100\n",
      "2499/2499 [==============================] - 0s 85us/step - loss: 0.3890 - acc: 0.8531 - val_loss: 0.3880 - val_acc: 0.8609\n",
      "Epoch 91/100\n",
      "2499/2499 [==============================] - 0s 69us/step - loss: 0.3885 - acc: 0.8531 - val_loss: 0.3873 - val_acc: 0.8609\n",
      "Epoch 92/100\n",
      "2499/2499 [==============================] - 0s 74us/step - loss: 0.3879 - acc: 0.8531 - val_loss: 0.3870 - val_acc: 0.8609\n",
      "Epoch 93/100\n",
      "2499/2499 [==============================] - 0s 75us/step - loss: 0.3874 - acc: 0.8531 - val_loss: 0.3863 - val_acc: 0.8609\n",
      "Epoch 94/100\n",
      "2499/2499 [==============================] - 0s 70us/step - loss: 0.3868 - acc: 0.8531 - val_loss: 0.3858 - val_acc: 0.8609\n",
      "Epoch 95/100\n",
      "2499/2499 [==============================] - 0s 73us/step - loss: 0.3864 - acc: 0.8531 - val_loss: 0.3857 - val_acc: 0.8609\n",
      "Epoch 96/100\n",
      "2499/2499 [==============================] - 0s 78us/step - loss: 0.3859 - acc: 0.8531 - val_loss: 0.3851 - val_acc: 0.8609\n",
      "Epoch 97/100\n",
      "2499/2499 [==============================] - 0s 86us/step - loss: 0.3857 - acc: 0.8531 - val_loss: 0.3852 - val_acc: 0.8609\n",
      "Epoch 98/100\n",
      "2499/2499 [==============================] - 0s 83us/step - loss: 0.3855 - acc: 0.8531 - val_loss: 0.3852 - val_acc: 0.8609\n",
      "Epoch 99/100\n",
      "2499/2499 [==============================] - 0s 87us/step - loss: 0.3850 - acc: 0.8531 - val_loss: 0.3845 - val_acc: 0.8609\n",
      "Epoch 100/100\n",
      "2499/2499 [==============================] - 0s 89us/step - loss: 0.3849 - acc: 0.8531 - val_loss: 0.3845 - val_acc: 0.8609\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "model_2 = Sequential()\n",
    "\n",
    "n_input = X_train.shape[1]\n",
    "n_hidden = n_input\n",
    "n_output = 1\n",
    "\n",
    "model_2.add(Dense(n_hidden, input_dim=n_input, activation='relu',\n",
    "                  kernel_regularizer=regularizers.l1_l2(0.01)))\n",
    "model_2.add(Dense(n_output, activation='sigmoid', \n",
    "                  kernel_regularizer=regularizers.l1_l2(0.01)))\n",
    "\n",
    "model_2.compile(loss='binary_crossentropy', optimizer='adam', \n",
    "                metrics=['acc'])\n",
    "\n",
    "history=model_2.fit(X_train, y_train, validation_data=(X_test, y_test), \n",
    "            epochs=100, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss=history.history['loss']\n",
    "test_loss=history.history['val_loss']\n",
    "plt.plot(train_loss,label='Training loss')\n",
    "plt.plot(test_loss,label='Test loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `L2` regularization and $\\lambda = 0.01$, **val_loss: **."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Resources\n",
    "- [Keras Regularization Documentation](https://keras.io/regularizers/)\n",
    "- [Kernel vs. Activity Regularizers](https://github.com/keras-team/keras/issues/3236)\n",
    "\n",
    "## Tensorflow Resources\n",
    "- [Implementation in Tensorflow](https://www.tensorflow.org/versions/r0.12/api_docs/python/contrib.layers/regularizers)\n",
    "- [Example in Tensorflow](http://www.ritchieng.com/machine-learning/deep-learning/tensorflow/regularization/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dropout'></a>\n",
    "## Regularization Method 2: Dropout\n",
    "---\n",
    "There's another method of regularizing our terms that is specifically designed for neural networks, called **dropout regularization.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we've constructed a neural network. We've decided on the number of layers we want and the number of nodes in each layer. (We might say that we've decided on the **topology** or **structure** of our network.)\n",
    "\n",
    "![](./assets/original_nn.jpeg)\n",
    "\n",
    "However, a densely connected network like this will almost certainly overfit. Our network is learning a parameter for every single connection.\n",
    "> In the above example, we have 55 parameters being learned - and this is a very simple network, all things considered.\n",
    "\n",
    "> We can overcome this by using **dropout regularization**. \n",
    "\n",
    "In dropout regularization, we randomly **drop** units (nodes) in our neural network ***during our training phase only***. We assign a probability of each node disappearing. Then, we essentially perform a coinflip for every node to turn that node \"on\" or \"off.\"\n",
    "\n",
    "Let's go through an example to illustrate this: For simplicity, we'll say we've assigned a 0.5 probability of keeping to every node in the network above. Then, for every node, we flip a coin, and if the coin lands on heads, the node remains, if it lands on tails, the node disappears. After we've done this for every node, we're left with a new network that looks something like this:\n",
    "\n",
    "![](./assets/after_dropout.jpeg)\n",
    "\n",
    "<!--\n",
    "Image sources: https://pgaleone.eu/deep-learning/regularization/2017/01/10/anaysis-of-dropout/\n",
    "Also, it seems, this site: http://cs231n.github.io/neural-networks-2/\n",
    "-->\n",
    "\n",
    "Let's explicitly lay out the general workflow you would follow:\n",
    "\n",
    "1. Specify the **topology** of your neural network.\n",
    "2. Initialize your weights and biases.\n",
    "3. Specify the \"keeping probabilities\" for every node. (Generally, we'll assign the same probability to all nodes in each layer and usually the same probability to all hidden layers.)\n",
    "4. Perform a \"coin flip\" for each node and drop out the chosen nodes.\n",
    "5. Run through one epoch of training.\n",
    "6. Repeat steps 4 and 5 for each epoch of training.\n",
    "\n",
    "**Check:** If I drop out a node during one of my epochs, does it disappear from my final network?\n",
    "\n",
    "#### So, what does this do?\n",
    "<!-- <br/> -->\n",
    "The intuition behind dropout is that, since each node has a probability of disappearing at any time, the neural network is disincentivized from allocating too much power to any one weight. It has a similar effect as imposing an L2 penalty: the magnitude of our weights shrinks.\n",
    "\n",
    "**Check:** What might be some potential problems with doing this?\n",
    "\n",
    "<!--\n",
    "expected values of nodes changes; induces bias\n",
    "-->\n",
    "\n",
    "#### Inverted Dropout\n",
    "\n",
    "In order to avoid any issues with the expected values of our nodes changing, we adjust our results accordingly by a method called **inverted dropout**.\n",
    "\n",
    "If we have a hidden layer with 100 nodes and each node has a 80% probability of being \"staying turned on,\" we only have 80% of those inputs to our node. As a result, we expect that the combined input to our node $z = b_0 + \\sum_{i=1}^pw_ix_i$ will be off by about 20%. (Those interested in probability and research might note that the Binomial distribution is a very convenient model for neural networks and dropout.)\n",
    "\n",
    "When using inverted dropout, we adjust $z$ by the \"keeping probability.\"\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "z_{original} &=& b_0 + \\sum_{i=1}^pw_ix_i \\\\\n",
    "\\Rightarrow z_{dropout} &=& b_0 + \\sum_{i\\in\\{included\\_nodes\\}}w_ix_i \\\\\n",
    "\\Rightarrow z_{inverted\\_dropout} &:=& z_{dropout} / 0.8 \\\\\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "#### Test time:\n",
    "\n",
    "So we've now run through every epoch of our training phase and we're ready to apply our neural network to our validation or testing data. Are we going to apply dropout to this data as well?\n",
    "\n",
    "**NO.**\n",
    "\n",
    "#### Best practices:\n",
    "\n",
    "- Don't set any keeping probabilities for layers you where you don't want to drop any nodes. (What might be examples of these layers?)\n",
    "<!--\n",
    "Input and output layers\n",
    "-->\n",
    "- You'll generally want to specify a single keeping probability on all the layers on which you want to apply dropout, instead of specifying different keeping probabilities for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2499 samples, validate on 834 samples\n",
      "Epoch 1/100\n",
      "2499/2499 [==============================] - 1s 207us/step - loss: 0.7027 - acc: 0.6779 - val_loss: 0.5488 - val_acc: 0.8261\n",
      "Epoch 2/100\n",
      "2499/2499 [==============================] - 0s 70us/step - loss: 0.5735 - acc: 0.7743 - val_loss: 0.4673 - val_acc: 0.8597\n",
      "Epoch 3/100\n",
      "2499/2499 [==============================] - 0s 72us/step - loss: 0.5077 - acc: 0.8155 - val_loss: 0.4219 - val_acc: 0.8609\n",
      "Epoch 4/100\n",
      "2499/2499 [==============================] - 0s 69us/step - loss: 0.4652 - acc: 0.8291 - val_loss: 0.3922 - val_acc: 0.8609\n",
      "Epoch 5/100\n",
      "2499/2499 [==============================] - 0s 68us/step - loss: 0.4283 - acc: 0.8403 - val_loss: 0.3713 - val_acc: 0.8621\n",
      "Epoch 6/100\n",
      "2499/2499 [==============================] - 0s 69us/step - loss: 0.4027 - acc: 0.8471 - val_loss: 0.3583 - val_acc: 0.8633\n",
      "Epoch 7/100\n",
      "2499/2499 [==============================] - 0s 71us/step - loss: 0.4040 - acc: 0.8547 - val_loss: 0.3479 - val_acc: 0.8681\n",
      "Epoch 8/100\n",
      "2499/2499 [==============================] - 0s 83us/step - loss: 0.3897 - acc: 0.8563 - val_loss: 0.3396 - val_acc: 0.8681\n",
      "Epoch 9/100\n",
      "2499/2499 [==============================] - 0s 81us/step - loss: 0.3671 - acc: 0.8583 - val_loss: 0.3333 - val_acc: 0.8681\n",
      "Epoch 10/100\n",
      "2499/2499 [==============================] - 0s 74us/step - loss: 0.3691 - acc: 0.8583 - val_loss: 0.3274 - val_acc: 0.8669\n",
      "Epoch 11/100\n",
      "2499/2499 [==============================] - 0s 69us/step - loss: 0.3721 - acc: 0.8587 - val_loss: 0.3243 - val_acc: 0.8693\n",
      "Epoch 12/100\n",
      "2499/2499 [==============================] - 0s 72us/step - loss: 0.3628 - acc: 0.8655 - val_loss: 0.3216 - val_acc: 0.8681\n",
      "Epoch 13/100\n",
      "2499/2499 [==============================] - 0s 72us/step - loss: 0.3543 - acc: 0.8655 - val_loss: 0.3180 - val_acc: 0.8717\n",
      "Epoch 14/100\n",
      "2499/2499 [==============================] - 0s 72us/step - loss: 0.3420 - acc: 0.8691 - val_loss: 0.3143 - val_acc: 0.8777\n",
      "Epoch 15/100\n",
      "2499/2499 [==============================] - 0s 69us/step - loss: 0.3257 - acc: 0.8824 - val_loss: 0.3106 - val_acc: 0.8765\n",
      "Epoch 16/100\n",
      "2499/2499 [==============================] - 0s 70us/step - loss: 0.3269 - acc: 0.8808 - val_loss: 0.3076 - val_acc: 0.8789\n",
      "Epoch 17/100\n",
      "2499/2499 [==============================] - 0s 69us/step - loss: 0.3306 - acc: 0.8768 - val_loss: 0.3060 - val_acc: 0.8801\n",
      "Epoch 18/100\n",
      "2499/2499 [==============================] - 0s 82us/step - loss: 0.3292 - acc: 0.8691 - val_loss: 0.3036 - val_acc: 0.8801\n",
      "Epoch 19/100\n",
      "2499/2499 [==============================] - 0s 87us/step - loss: 0.3198 - acc: 0.8816 - val_loss: 0.3010 - val_acc: 0.8813\n",
      "Epoch 20/100\n",
      "2499/2499 [==============================] - 0s 82us/step - loss: 0.3233 - acc: 0.8756 - val_loss: 0.2994 - val_acc: 0.8837\n",
      "Epoch 21/100\n",
      "2499/2499 [==============================] - 0s 75us/step - loss: 0.3140 - acc: 0.8812 - val_loss: 0.2971 - val_acc: 0.8861\n",
      "Epoch 22/100\n",
      "2499/2499 [==============================] - 0s 74us/step - loss: 0.3032 - acc: 0.8808 - val_loss: 0.2946 - val_acc: 0.8873\n",
      "Epoch 23/100\n",
      "2499/2499 [==============================] - 0s 73us/step - loss: 0.3116 - acc: 0.8872 - val_loss: 0.2930 - val_acc: 0.8849\n",
      "Epoch 24/100\n",
      "2499/2499 [==============================] - 0s 80us/step - loss: 0.3116 - acc: 0.8792 - val_loss: 0.2908 - val_acc: 0.8849\n",
      "Epoch 25/100\n",
      "2499/2499 [==============================] - 0s 80us/step - loss: 0.2939 - acc: 0.8904 - val_loss: 0.2891 - val_acc: 0.8861\n",
      "Epoch 26/100\n",
      "2499/2499 [==============================] - 0s 77us/step - loss: 0.2988 - acc: 0.8880 - val_loss: 0.2889 - val_acc: 0.8861\n",
      "Epoch 27/100\n",
      "2499/2499 [==============================] - 0s 79us/step - loss: 0.3044 - acc: 0.8900 - val_loss: 0.2865 - val_acc: 0.8897\n",
      "Epoch 28/100\n",
      "2499/2499 [==============================] - 0s 77us/step - loss: 0.2961 - acc: 0.8952 - val_loss: 0.2852 - val_acc: 0.8921\n",
      "Epoch 29/100\n",
      "2499/2499 [==============================] - 0s 79us/step - loss: 0.2963 - acc: 0.8892 - val_loss: 0.2850 - val_acc: 0.8861\n",
      "Epoch 30/100\n",
      "2499/2499 [==============================] - 0s 76us/step - loss: 0.2783 - acc: 0.8984 - val_loss: 0.2827 - val_acc: 0.8921\n",
      "Epoch 31/100\n",
      "2499/2499 [==============================] - 0s 78us/step - loss: 0.2919 - acc: 0.8976 - val_loss: 0.2817 - val_acc: 0.8969\n",
      "Epoch 32/100\n",
      "2499/2499 [==============================] - 0s 77us/step - loss: 0.2965 - acc: 0.8888 - val_loss: 0.2810 - val_acc: 0.8957\n",
      "Epoch 33/100\n",
      "2499/2499 [==============================] - 0s 69us/step - loss: 0.2955 - acc: 0.8920 - val_loss: 0.2794 - val_acc: 0.8993\n",
      "Epoch 34/100\n",
      "2499/2499 [==============================] - 0s 69us/step - loss: 0.2820 - acc: 0.8960 - val_loss: 0.2781 - val_acc: 0.8993\n",
      "Epoch 35/100\n",
      "2499/2499 [==============================] - 0s 68us/step - loss: 0.2760 - acc: 0.8940 - val_loss: 0.2779 - val_acc: 0.9017\n",
      "Epoch 36/100\n",
      "2499/2499 [==============================] - 0s 78us/step - loss: 0.2771 - acc: 0.8984 - val_loss: 0.2758 - val_acc: 0.9041\n",
      "Epoch 37/100\n",
      "2499/2499 [==============================] - 0s 73us/step - loss: 0.2806 - acc: 0.8980 - val_loss: 0.2756 - val_acc: 0.9005\n",
      "Epoch 38/100\n",
      "2499/2499 [==============================] - 0s 69us/step - loss: 0.2766 - acc: 0.8992 - val_loss: 0.2750 - val_acc: 0.9029\n",
      "Epoch 39/100\n",
      "2499/2499 [==============================] - 0s 76us/step - loss: 0.2753 - acc: 0.8940 - val_loss: 0.2742 - val_acc: 0.9017\n",
      "Epoch 40/100\n",
      "2499/2499 [==============================] - 0s 63us/step - loss: 0.2795 - acc: 0.9004 - val_loss: 0.2722 - val_acc: 0.9029\n",
      "Epoch 41/100\n",
      "2499/2499 [==============================] - 0s 65us/step - loss: 0.2807 - acc: 0.8980 - val_loss: 0.2725 - val_acc: 0.9029\n",
      "Epoch 42/100\n",
      "2499/2499 [==============================] - 0s 63us/step - loss: 0.2756 - acc: 0.9004 - val_loss: 0.2708 - val_acc: 0.9017\n",
      "Epoch 43/100\n",
      "2499/2499 [==============================] - 0s 60us/step - loss: 0.2791 - acc: 0.8960 - val_loss: 0.2712 - val_acc: 0.9017\n",
      "Epoch 44/100\n",
      "2499/2499 [==============================] - 0s 61us/step - loss: 0.2685 - acc: 0.9012 - val_loss: 0.2710 - val_acc: 0.9029\n",
      "Epoch 45/100\n",
      "2499/2499 [==============================] - 0s 65us/step - loss: 0.2713 - acc: 0.9000 - val_loss: 0.2701 - val_acc: 0.9029\n",
      "Epoch 46/100\n",
      "2499/2499 [==============================] - 0s 65us/step - loss: 0.2685 - acc: 0.9024 - val_loss: 0.2694 - val_acc: 0.9017\n",
      "Epoch 47/100\n",
      "2499/2499 [==============================] - 0s 65us/step - loss: 0.2649 - acc: 0.9012 - val_loss: 0.2693 - val_acc: 0.9029\n",
      "Epoch 48/100\n",
      "2499/2499 [==============================] - 0s 68us/step - loss: 0.2773 - acc: 0.8956 - val_loss: 0.2698 - val_acc: 0.9017\n",
      "Epoch 49/100\n",
      "2499/2499 [==============================] - 0s 67us/step - loss: 0.2640 - acc: 0.9032 - val_loss: 0.2679 - val_acc: 0.9029\n",
      "Epoch 50/100\n",
      "2499/2499 [==============================] - 0s 69us/step - loss: 0.2650 - acc: 0.9004 - val_loss: 0.2663 - val_acc: 0.9065\n",
      "Epoch 51/100\n",
      "2499/2499 [==============================] - 0s 66us/step - loss: 0.2785 - acc: 0.9004 - val_loss: 0.2665 - val_acc: 0.9053\n",
      "Epoch 52/100\n",
      "2499/2499 [==============================] - 0s 70us/step - loss: 0.2691 - acc: 0.9028 - val_loss: 0.2653 - val_acc: 0.9053\n",
      "Epoch 53/100\n",
      "2499/2499 [==============================] - 0s 73us/step - loss: 0.2673 - acc: 0.8996 - val_loss: 0.2651 - val_acc: 0.9101\n",
      "Epoch 54/100\n",
      "2499/2499 [==============================] - 0s 77us/step - loss: 0.2684 - acc: 0.9004 - val_loss: 0.2647 - val_acc: 0.9125\n",
      "Epoch 55/100\n",
      "2499/2499 [==============================] - 0s 75us/step - loss: 0.2670 - acc: 0.9008 - val_loss: 0.2640 - val_acc: 0.9089\n",
      "Epoch 56/100\n",
      "2499/2499 [==============================] - 0s 72us/step - loss: 0.2632 - acc: 0.9100 - val_loss: 0.2637 - val_acc: 0.9101\n",
      "Epoch 57/100\n",
      "2499/2499 [==============================] - 0s 69us/step - loss: 0.2684 - acc: 0.9008 - val_loss: 0.2641 - val_acc: 0.9041\n",
      "Epoch 58/100\n",
      "2499/2499 [==============================] - 0s 75us/step - loss: 0.2677 - acc: 0.8988 - val_loss: 0.2628 - val_acc: 0.9053\n",
      "Epoch 59/100\n",
      "2499/2499 [==============================] - 0s 75us/step - loss: 0.2680 - acc: 0.9012 - val_loss: 0.2630 - val_acc: 0.9053\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2499/2499 [==============================] - 0s 75us/step - loss: 0.2524 - acc: 0.9080 - val_loss: 0.2617 - val_acc: 0.9041\n",
      "Epoch 61/100\n",
      "2499/2499 [==============================] - 0s 79us/step - loss: 0.2574 - acc: 0.9052 - val_loss: 0.2610 - val_acc: 0.9053\n",
      "Epoch 62/100\n",
      "2499/2499 [==============================] - 0s 81us/step - loss: 0.2586 - acc: 0.9028 - val_loss: 0.2615 - val_acc: 0.9053\n",
      "Epoch 63/100\n",
      "2499/2499 [==============================] - 0s 81us/step - loss: 0.2635 - acc: 0.9048 - val_loss: 0.2610 - val_acc: 0.9065\n",
      "Epoch 64/100\n",
      "2499/2499 [==============================] - 0s 74us/step - loss: 0.2583 - acc: 0.9028 - val_loss: 0.2607 - val_acc: 0.9041\n",
      "Epoch 65/100\n",
      "2499/2499 [==============================] - 0s 66us/step - loss: 0.2600 - acc: 0.8972 - val_loss: 0.2599 - val_acc: 0.9053\n",
      "Epoch 66/100\n",
      "2499/2499 [==============================] - 0s 68us/step - loss: 0.2528 - acc: 0.9072 - val_loss: 0.2590 - val_acc: 0.9077\n",
      "Epoch 67/100\n",
      "2499/2499 [==============================] - 0s 84us/step - loss: 0.2556 - acc: 0.9028 - val_loss: 0.2588 - val_acc: 0.9053\n",
      "Epoch 68/100\n",
      "2499/2499 [==============================] - 0s 89us/step - loss: 0.2581 - acc: 0.9036 - val_loss: 0.2573 - val_acc: 0.9053\n",
      "Epoch 69/100\n",
      "2499/2499 [==============================] - 0s 75us/step - loss: 0.2649 - acc: 0.9032 - val_loss: 0.2569 - val_acc: 0.9065\n",
      "Epoch 70/100\n",
      "2499/2499 [==============================] - 0s 73us/step - loss: 0.2490 - acc: 0.9088 - val_loss: 0.2563 - val_acc: 0.9053\n",
      "Epoch 71/100\n",
      "2499/2499 [==============================] - 0s 81us/step - loss: 0.2603 - acc: 0.9068 - val_loss: 0.2552 - val_acc: 0.9077\n",
      "Epoch 72/100\n",
      "2499/2499 [==============================] - 0s 76us/step - loss: 0.2623 - acc: 0.9052 - val_loss: 0.2556 - val_acc: 0.9077\n",
      "Epoch 73/100\n",
      "2499/2499 [==============================] - 0s 77us/step - loss: 0.2555 - acc: 0.9052 - val_loss: 0.2550 - val_acc: 0.9089\n",
      "Epoch 74/100\n",
      "2499/2499 [==============================] - 0s 79us/step - loss: 0.2498 - acc: 0.9072 - val_loss: 0.2549 - val_acc: 0.9089\n",
      "Epoch 75/100\n",
      "2499/2499 [==============================] - 0s 73us/step - loss: 0.2586 - acc: 0.9028 - val_loss: 0.2549 - val_acc: 0.9053\n",
      "Epoch 76/100\n",
      "2499/2499 [==============================] - 0s 92us/step - loss: 0.2513 - acc: 0.9056 - val_loss: 0.2550 - val_acc: 0.9041\n",
      "Epoch 77/100\n",
      "2499/2499 [==============================] - 0s 80us/step - loss: 0.2594 - acc: 0.9000 - val_loss: 0.2541 - val_acc: 0.9065\n",
      "Epoch 78/100\n",
      "2499/2499 [==============================] - 0s 83us/step - loss: 0.2495 - acc: 0.9084 - val_loss: 0.2538 - val_acc: 0.9089\n",
      "Epoch 79/100\n",
      "2499/2499 [==============================] - 0s 78us/step - loss: 0.2514 - acc: 0.9084 - val_loss: 0.2532 - val_acc: 0.9065\n",
      "Epoch 80/100\n",
      "2499/2499 [==============================] - 0s 82us/step - loss: 0.2511 - acc: 0.9124 - val_loss: 0.2527 - val_acc: 0.9101\n",
      "Epoch 81/100\n",
      "2499/2499 [==============================] - 0s 84us/step - loss: 0.2531 - acc: 0.9060 - val_loss: 0.2519 - val_acc: 0.9125\n",
      "Epoch 82/100\n",
      "2499/2499 [==============================] - 0s 69us/step - loss: 0.2493 - acc: 0.9024 - val_loss: 0.2540 - val_acc: 0.9089\n",
      "Epoch 83/100\n",
      "2499/2499 [==============================] - 0s 79us/step - loss: 0.2563 - acc: 0.9048 - val_loss: 0.2526 - val_acc: 0.9101\n",
      "Epoch 84/100\n",
      "2499/2499 [==============================] - 0s 93us/step - loss: 0.2486 - acc: 0.9056 - val_loss: 0.2521 - val_acc: 0.9149\n",
      "Epoch 85/100\n",
      "2499/2499 [==============================] - 0s 89us/step - loss: 0.2539 - acc: 0.9068 - val_loss: 0.2527 - val_acc: 0.9125\n",
      "Epoch 86/100\n",
      "2499/2499 [==============================] - 0s 91us/step - loss: 0.2517 - acc: 0.9092 - val_loss: 0.2506 - val_acc: 0.9113\n",
      "Epoch 87/100\n",
      "2499/2499 [==============================] - 0s 92us/step - loss: 0.2437 - acc: 0.9116 - val_loss: 0.2486 - val_acc: 0.9137\n",
      "Epoch 88/100\n",
      "2499/2499 [==============================] - 0s 84us/step - loss: 0.2492 - acc: 0.9140 - val_loss: 0.2482 - val_acc: 0.9149\n",
      "Epoch 89/100\n",
      "2499/2499 [==============================] - 0s 84us/step - loss: 0.2425 - acc: 0.9152 - val_loss: 0.2480 - val_acc: 0.9173\n",
      "Epoch 90/100\n",
      "2499/2499 [==============================] - 0s 83us/step - loss: 0.2453 - acc: 0.9084 - val_loss: 0.2481 - val_acc: 0.9125\n",
      "Epoch 91/100\n",
      "2499/2499 [==============================] - 0s 102us/step - loss: 0.2519 - acc: 0.9096 - val_loss: 0.2470 - val_acc: 0.9137\n",
      "Epoch 92/100\n",
      "2499/2499 [==============================] - 0s 82us/step - loss: 0.2483 - acc: 0.9052 - val_loss: 0.2463 - val_acc: 0.9149\n",
      "Epoch 93/100\n",
      "2499/2499 [==============================] - 0s 88us/step - loss: 0.2468 - acc: 0.9076 - val_loss: 0.2472 - val_acc: 0.9137\n",
      "Epoch 94/100\n",
      "2499/2499 [==============================] - 0s 81us/step - loss: 0.2536 - acc: 0.9096 - val_loss: 0.2473 - val_acc: 0.9149\n",
      "Epoch 95/100\n",
      "2499/2499 [==============================] - 0s 77us/step - loss: 0.2467 - acc: 0.9064 - val_loss: 0.2458 - val_acc: 0.9173\n",
      "Epoch 96/100\n",
      "2499/2499 [==============================] - 0s 81us/step - loss: 0.2478 - acc: 0.9108 - val_loss: 0.2455 - val_acc: 0.9161\n",
      "Epoch 97/100\n",
      "2499/2499 [==============================] - 0s 77us/step - loss: 0.2453 - acc: 0.9120 - val_loss: 0.2460 - val_acc: 0.9161\n",
      "Epoch 98/100\n",
      "2499/2499 [==============================] - 0s 74us/step - loss: 0.2441 - acc: 0.9144 - val_loss: 0.2457 - val_acc: 0.9173\n",
      "Epoch 99/100\n",
      "2499/2499 [==============================] - 0s 74us/step - loss: 0.2432 - acc: 0.9136 - val_loss: 0.2443 - val_acc: 0.9185\n",
      "Epoch 100/100\n",
      "2499/2499 [==============================] - 0s 79us/step - loss: 0.2464 - acc: 0.9136 - val_loss: 0.2445 - val_acc: 0.9185\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "model_3 = Sequential()\n",
    "\n",
    "n_input = X_train.shape[1]\n",
    "n_hidden = n_input\n",
    "n_output = 1\n",
    "\n",
    "model_3.add(Dense(n_hidden, input_dim=n_input, activation='relu'))\n",
    "model_3.add(Dropout(0.5)) # dropping nodes in the hidden layer\n",
    "model_3.add(Dense(n_output, activation='sigmoid'))\n",
    "\n",
    "model_3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "history = model_3.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "                      epochs=100, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a29f613c8>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss_3 = history.history['loss']\n",
    "test_loss_3 = history.history['val_loss']\n",
    "plt.plot(train_loss_3, label='Training loss')\n",
    "plt.plot(test_loss_3, label='Testing loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Dropout, **val_loss: **."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Resources\n",
    "- [Keras Dropout Documentation](https://keras.io/layers/core/#dropout)\n",
    "\n",
    "## Tensorflow Resources\n",
    "- [Tensorflow documentation](https://www.tensorflow.org/api_docs/python/tf/nn/dropout)\n",
    "- [List of examples in Tensorflow](https://programtalk.com/python-examples/tensorflow.nn.dropout/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='stopping'></a>\n",
    "## Regularization Method 3: Early Stopping\n",
    "---\n",
    "The third method of regularization that we'll discuss today is called early stopping.\n",
    "</br>\n",
    "If we run though all our epochs of training and plot both our training and validation error, we'll typically see something like this:\n",
    "\n",
    "![](./assets/train-val-error-reduced.png)\n",
    "*source: [Prechelt, 1997](http://page.mi.fu-berlin.de/prechelt/Biblio/stop_tricks1997.pdf) *\n",
    "\n",
    "**Check:** What is happening in this plot?\n",
    "\n",
    "Early stopping does exactly what its name implies: it stop the training process early. Instead of continuing training through every epoch, once the validation error begins to increase, our algorithm stops because it has (in theory) found the minimum for the validation loss.\n",
    "\n",
    "This might seem like a simple and robust solution to overfitting, but it can run into problems.\n",
    "\n",
    "<details>\n",
    "![](./assets/validation-error-real.png)\n",
    "</details>\n",
    "\n",
    "There is debate over how often this problem occurs. You can generally plot both the training and validation loss, see if you're getting multiple optima. If you are, there are multiple suggested techniques to combat this problem in the [paper reference above](http://page.mi.fu-berlin.de/prechelt/Biblio/stop_tricks1997.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2499 samples, validate on 834 samples\n",
      "Epoch 1/100\n",
      "2499/2499 [==============================] - 1s 252us/step - loss: 0.7059 - acc: 0.5434 - val_loss: 0.5704 - val_acc: 0.6942\n",
      "Epoch 2/100\n",
      "2499/2499 [==============================] - 0s 70us/step - loss: 0.5156 - acc: 0.7791 - val_loss: 0.4628 - val_acc: 0.8393\n",
      "Epoch 3/100\n",
      "2499/2499 [==============================] - 0s 78us/step - loss: 0.4415 - acc: 0.8547 - val_loss: 0.4104 - val_acc: 0.8657\n",
      "Epoch 4/100\n",
      "2499/2499 [==============================] - 0s 73us/step - loss: 0.3990 - acc: 0.8631 - val_loss: 0.3784 - val_acc: 0.8705\n",
      "Epoch 5/100\n",
      "2499/2499 [==============================] - 0s 70us/step - loss: 0.3715 - acc: 0.8627 - val_loss: 0.3600 - val_acc: 0.8693\n",
      "Epoch 6/100\n",
      "2499/2499 [==============================] - 0s 76us/step - loss: 0.3541 - acc: 0.8639 - val_loss: 0.3478 - val_acc: 0.8681\n",
      "Epoch 7/100\n",
      "2499/2499 [==============================] - 0s 79us/step - loss: 0.3418 - acc: 0.8655 - val_loss: 0.3377 - val_acc: 0.8729\n",
      "Epoch 8/100\n",
      "2499/2499 [==============================] - 0s 80us/step - loss: 0.3304 - acc: 0.8655 - val_loss: 0.3297 - val_acc: 0.8741\n",
      "Epoch 9/100\n",
      "2499/2499 [==============================] - 0s 81us/step - loss: 0.3194 - acc: 0.8699 - val_loss: 0.3214 - val_acc: 0.8753\n",
      "Epoch 10/100\n",
      "2499/2499 [==============================] - 0s 78us/step - loss: 0.3091 - acc: 0.8747 - val_loss: 0.3143 - val_acc: 0.8777\n",
      "Epoch 11/100\n",
      "2499/2499 [==============================] - 0s 79us/step - loss: 0.2980 - acc: 0.8780 - val_loss: 0.3063 - val_acc: 0.8837\n",
      "Epoch 12/100\n",
      "2499/2499 [==============================] - 0s 81us/step - loss: 0.2873 - acc: 0.8896 - val_loss: 0.2988 - val_acc: 0.8897\n",
      "Epoch 13/100\n",
      "2499/2499 [==============================] - 0s 81us/step - loss: 0.2777 - acc: 0.8928 - val_loss: 0.2918 - val_acc: 0.8945\n",
      "Epoch 14/100\n",
      "2499/2499 [==============================] - 0s 82us/step - loss: 0.2686 - acc: 0.8980 - val_loss: 0.2858 - val_acc: 0.9005\n",
      "Epoch 15/100\n",
      "2499/2499 [==============================] - 0s 83us/step - loss: 0.2608 - acc: 0.9016 - val_loss: 0.2815 - val_acc: 0.9077\n",
      "Epoch 16/100\n",
      "2499/2499 [==============================] - 0s 73us/step - loss: 0.2537 - acc: 0.9056 - val_loss: 0.2775 - val_acc: 0.9089\n",
      "Epoch 17/100\n",
      "2499/2499 [==============================] - 0s 75us/step - loss: 0.2479 - acc: 0.9076 - val_loss: 0.2744 - val_acc: 0.9077\n",
      "Epoch 18/100\n",
      "2499/2499 [==============================] - 0s 76us/step - loss: 0.2438 - acc: 0.9096 - val_loss: 0.2725 - val_acc: 0.9113\n",
      "Epoch 19/100\n",
      "2499/2499 [==============================] - 0s 81us/step - loss: 0.2393 - acc: 0.9136 - val_loss: 0.2704 - val_acc: 0.9113\n",
      "Epoch 20/100\n",
      "2499/2499 [==============================] - 0s 77us/step - loss: 0.2356 - acc: 0.9124 - val_loss: 0.2686 - val_acc: 0.9113\n",
      "Epoch 21/100\n",
      "2499/2499 [==============================] - 0s 76us/step - loss: 0.2326 - acc: 0.9184 - val_loss: 0.2667 - val_acc: 0.9113\n",
      "Epoch 22/100\n",
      "2499/2499 [==============================] - 0s 72us/step - loss: 0.2295 - acc: 0.9208 - val_loss: 0.2644 - val_acc: 0.9149\n",
      "Epoch 23/100\n",
      "2499/2499 [==============================] - 0s 74us/step - loss: 0.2270 - acc: 0.9208 - val_loss: 0.2633 - val_acc: 0.9173\n",
      "Epoch 24/100\n",
      "2499/2499 [==============================] - 0s 79us/step - loss: 0.2248 - acc: 0.9224 - val_loss: 0.2632 - val_acc: 0.9149\n",
      "Epoch 25/100\n",
      "2499/2499 [==============================] - 0s 70us/step - loss: 0.2222 - acc: 0.9236 - val_loss: 0.2619 - val_acc: 0.9173\n",
      "Epoch 26/100\n",
      "2499/2499 [==============================] - 0s 72us/step - loss: 0.2204 - acc: 0.9268 - val_loss: 0.2607 - val_acc: 0.9149\n",
      "Epoch 27/100\n",
      "2499/2499 [==============================] - 0s 77us/step - loss: 0.2185 - acc: 0.9240 - val_loss: 0.2603 - val_acc: 0.9125\n",
      "Epoch 28/100\n",
      "2499/2499 [==============================] - 0s 72us/step - loss: 0.2177 - acc: 0.9260 - val_loss: 0.2604 - val_acc: 0.9125\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "model_4 = Sequential()\n",
    "\n",
    "n_input = X_train.shape[1]\n",
    "n_hidden = n_input\n",
    "n_output = 1\n",
    "\n",
    "model_4.add(Dense(n_hidden, input_dim=n_input, activation='relu'))\n",
    "model_4.add(Dense(n_output, activation='sigmoid'))\n",
    "\n",
    "model_4.compile(loss='binary_crossentropy', optimizer='adam', \n",
    "                metrics=['acc'])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0)\n",
    "\n",
    "\n",
    "history = model_4.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "                      epochs=100, batch_size=None,\n",
    "                      callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss_4 = history.history['loss']\n",
    "test_loss_4 = history.history['val_loss']\n",
    "plt.plot(train_loss_4, label='Training loss')\n",
    "plt.plot(test_loss_4, label='Testing loss')\n",
    "plt.legend();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With early stopping, **val_loss: **."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Resources\n",
    "- [Keras EarlyStopping Documentation](https://keras.io/callbacks/#earlystopping)\n",
    "- [Keras EarlyStopping Example](http://parneetk.github.io/blog/neural-networks-in-keras/)\n",
    "\n",
    "## Tensorflow Resources\n",
    "- [Tensorflow.Keras.callbacks.EarlyStopping Documentation](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization\n",
    "---\n",
    "Now we're going to talk about ways to speed up the process of optimization. Surprisingly, local optima are not often problems with neural networks; there's a much larger issue with \"plateaus,\" or areas where the derivative is approximately 0. This makes learning much slower.\n",
    "\n",
    "## Quick Fixes:\n",
    "1. **Feature Scaling:** As we've discussed before, feature scaling can speed up the process of gradient descent. Because gradient descent works \"geometrically,\" the scales of our $w$ values have a large impact on how quickly our parameters converge to the true value. Since we're working with many parameters (often 1,000 or more), scaling our features will speed up optimization.\n",
    "\n",
    "2. **Mini-batch Gradient Descent:** In `sklearn`, we use `.fit()` to estimate the parameters in our model. We do the same in neural networks, but if we're working with a very large data set (as is common in neural networks), passing data through our network will cause learning to be slow. By specifying a `batch_size` within the `.fit()` method, we can expedite our learning.\n",
    "    - `batch_size`: Integer or `None`. Number of samples per gradient update. If unspecified, it will default to 32.\n",
    "    \n",
    "**Note:** Mini batches will usually be a power of 2 (32, 64, 128, 256, 512) due to the fact that computers operate in base 2.\n",
    "\n",
    "- [Overview of the three types of gradient descent: batch, stochastic, mini-batch](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='momentum'></a>\n",
    "## Gradient Descent with Momentum\n",
    "\n",
    "One problem we notice with mini-batch gradient descent compared to batch gradient descent is that it tends to oscillate more, due to the fact that at each iteration it is being fed less training data than batch gradient descent.\n",
    "\n",
    "![](./assets/grad-descent.png)\n",
    "![](./assets/mini-grad-descent.png)\n",
    "\n",
    "*[source](http://pengcheng.tech/2017/09/28/gradient-descent-momentum-and-adam/)*\n",
    "\n",
    "Ideally, we'd like to take advantage of the speed and memory efficiency of mini-batch gradient descent, without the oscillations. We can use the idea of **momentum** to help us out here. (Momentum is based on [exponentially weighted moving averages](https://www.compose.com/articles/metrics-maven-calculating-an-exponentially-weighted-moving-average-in-postgresql/), which causes our oscillations to largely cancel each other out.) \n",
    "\n",
    "Typically, when updating our parameters, we'll follow this formula:\n",
    "\n",
    "$$W = W -\\alpha\\frac{\\partial \\text{loss}}{\\partial W}$$  \n",
    "$$b = b - \\alpha\\frac{\\partial \\text{loss}}{\\partial b}$$\n",
    "\n",
    "We're going to modify this formula to this form:\n",
    "\n",
    "$$W = W -\\alpha V_{\\partial W}$$  \n",
    "$$b = b - \\alpha V_{\\partial b}$$\n",
    "\n",
    "Where \n",
    "$$V_{\\partial W} = \\beta V_{\\partial W - 1} + (1-\\beta)\\frac{\\partial \\text{loss}}{\\partial W}$$\n",
    "and\n",
    "$$V_{\\partial b} = \\beta V_{\\partial b - 1} + (1-\\beta)\\frac{\\partial \\text{loss}}{\\partial b}$$\n",
    "\n",
    "When implementing gradient descent with momentum, you'll have two hyperparameters, $\\alpha$ and $\\beta$. From a practical point of view, $\\beta$ is typically $0.9$, but you can test out other values if you'd like.\n",
    "\n",
    "### ADAM Optimization\n",
    "\n",
    "The optimization algorithm you'll likely use when implementing your feed-forward neural network is called ADAM (Adaptive Moment Estimation). It is a combination of gradient descent with momentum and another optimization method called RMSProp (Root Mean Square Propagation). For the sake of this lesson, we won't cover how ADAM works, but with gradient descent with momentum as building block, the ADAM optimization is not too far off, as it largely relies on the concept of momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "One method of minimizing the risk of overfitting is to gather more data. While this is usually very costly, we may sometimes be able to take our existing data to generate substantially more data.\n",
    "- Images: Reflect, crop, random rotations or distortions, adjust lighting.\n",
    "    - [The Effectiveness of Data Augmentation in Image Classification using Deep Learning](http://cs231n.stanford.edu/reports/2017/pdfs/300.pdf)\n",
    "- Non-Images: SMOTE (Synthetic Minority Over-Sampling Technique)\n",
    "    - [SMOTE Paper](https://arxiv.org/abs/1106.1813)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusion'></a>\n",
    "# Conclusion\n",
    "\n",
    "Today, we learned about three different methods of regularizing our neural networks: `L2` regularization, dropout, and early stopping.\n",
    "\n",
    "## Machine Learning Workflow\n",
    "\n",
    "As promised, managing bias and variance takes a lot of our attention. If our bias or variance are high, it's likely that our model isn't performing as well as it could.\n",
    "\n",
    "A workflow for how you should address this (in the context of neural networks and beyond) is as follows:\n",
    "\n",
    "- Do we have high bias? (i.e. are we performing poorly on our training set?)\n",
    "    - If so:\n",
    "        - let's build a more complex model / bigger network!\n",
    "        - let's consider a new architecture for our neural network!\n",
    "        - let's train longer!\n",
    "- Do we have high variance? (i.e. are we performing poorly on our test/holdout set?)\n",
    "    - If so:\n",
    "        - let's gather more data!\n",
    "            - Usually very difficult, but we should use \"data augmentation\" if we can!\n",
    "        - let's build a simpler model / smaller network!\n",
    "        - let's consider a new architecture for our neural network!\n",
    "        - let's regularize!\n",
    "    - Once we're satisfied, return to the bias question and repeat.\n",
    "    \n",
    "**Note:** Before deep learning, most tools for handling high bias or high variance adversely affected the other. However, depending on the amount of data we have and how complex our network is, it's often the case that we can drastically reduce variance with out affecting bias.\n",
    "\n",
    "We also learned about Mini-batch Gradient Descent and Gradient Descent with Momentum. You will almost always work with mini batches. And when optimizing, you will generally use a method called Adam Optimization that is built on a combination of Gradient Descent with Momentum and another optimization technique called RMSProp.\n",
    "\n",
    "We also briefly discussed data augmentation as a method for increasing our sample sie.\n",
    "\n",
    "<a id='references'></a>\n",
    "## References and Resources:\n",
    "\n",
    "- [DeepLearning.ai](https://www.deeplearning.ai/), Andrew Ng's Coursera course on Deep Learning\n",
    "  - The videos from this course are on a [YouTube Channel](https://www.youtube.com/channel/UCcIXc5mJsHVYTZR1maL5l9w/featured)   \n",
    "<br>\n",
    "- [Deep Learning Book](http://www.deeplearningbook.org/), textbook written by Ian Goodfellow, creator of Generative Adversarial Networks (GANs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
