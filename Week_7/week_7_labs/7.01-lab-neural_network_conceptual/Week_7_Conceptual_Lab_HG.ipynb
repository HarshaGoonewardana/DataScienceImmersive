{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Lab\n",
    "\n",
    "In this lab, we'll be exploring a visual proof of the universal approximation theorem and building (from scratch) a neural network that will approximate a pretty ridiculous function.\n",
    "\n",
    "Head over to [this site](http://neuralnetworksanddeeplearning.com/chap4.html) and read from the beginning of the page until the \"Many Input Variables\" section. (You do not need to read the \"Many Input Variables\" section and beyond but are certainly welcome to do so!) You'll read the introduction, the \"Two Caveats\" section, and the \"Universality with One Input and One Output\" section.\n",
    "\n",
    "Your answers to problems 1-5 should come from directly this reading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 1**: Summarize the Universal Approximation Theorem. (Don't copy it; use your own words!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### A simple neural network with even only a single layer given enough information can provide an acceptable approximation of a function. The closeness of the approximation is positively related to the number of neurons in the hiddel layer(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 2**: Summarize the two caveats the author uses to describe the statement \"a neural network can compute any function.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Neural networks only work in a very sepecialized conditions. These are that the input data needs to be continous and that the output is only an approximation not a definite answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 3:** For a sigmoidal activation function to closely resemble a step function, how would you describe the value of $w$? What constraints exist on the value of $b$? How do we calculate $s$? What does the value of $s$ indicate?\n",
    "\n",
    "Try playing around with the applets on the page to test how different parts of the perceptron affect the output. This should be helpful in answering the questions above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### the bias amount acts to mitigate the value of the parameter or weight. Interestingly the bias only moves the output function does not change the shape of it. The input weights need to start with a high value as not to get overwhelmed by the randomly assigned bias during the first pass.The existential purpose of the bias is to provide a foundation upon which for the w to balance itself. Therefors the bias can not be bigger than w. s is the ratio between w and b also known as a step and is calculated by w/b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 4**: When the author wants us to approximate $f(x)=0.2+0.4x^2+0.3x\\sin(15x)+0.05\\cos(50x)$ with a neural network, the function on the applet where we manipulate the values of $h_i$ is not $f(x)$. It's a different function. What is this function, and why are we working with this one instead of $f(x)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### the NN is using a sigmoid activation function and cranking the weights to extremes to flattern the output and calling it a step function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 5**: The author asks you to find values of $h_i$ that make your neural network closely approximate $\\sigma^{-1}\\circ f(x)$. Record your values of $h_i$ here and your best \"average deviation\" score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[-1.2, -1.7, -.8, -.9, 1.3], average deviation = .42\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 6**: Build the neural network from your work in Problem 5 here.\n",
    "\n",
    "A few things to keep in mind:\n",
    "* How many inputs are there? \n",
    "* How many outputs are there?\n",
    "* How many neurons are in the hidden layer?\n",
    "* In order to create step functions between 0 and 0.2, 0.2 and 0.4, etc., what does this suggest about the activation function in these neurons? Note that these activation functions will be different, but related.\n",
    "* What do the values of $h_i$ represent?\n",
    "\n",
    "Check out the Neural Networks I notes for an implementation in NumPy; you should be able to use this as a starting point for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [0.0, 0.2, 0.2, 0.4, 0.4, 0.6, 0.6, 0.8, 0.8, 1.0]\n",
    "layer1_weight = 1000\n",
    "layer_1_biases = [-layer1_weight*step for step in steps] # b = -ws\n",
    "final_weights = [-1.2, 1.2, -1.4, 1.4, -.3, .3, -1, 1, 1.2, -1.2] # paired from height of steps\n",
    "output_bias = 0\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def my_neural_net(x):\n",
    "    input_value = x\n",
    "    z = [input_value * layer1_weight + bias for bias in layer_1_biases]\n",
    "    activation = [sigmoid(z_i) for z_i in z]\n",
    "    \n",
    "    z_out = [i*j for i, j in zip(activation, final_weights)]\n",
    "    \n",
    "    return(np.sum(z_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 7**: Once you've built the neural network, use `np.linspace` to generate 1000 values of $x$ between 0 and 1 and use the `pynverse` [library](https://pypi.python.org/pypi/pynverse) to manually estimate the performance of your neural network using mean squared error.\n",
    "\n",
    "Recall that mean squared error is given by:\n",
    "\n",
    "$$\n",
    "\\frac{1}{n}\\sum_{i=1}^n (\\hat{y}-y)^2\n",
    "$$\n",
    "\n",
    "\n",
    "* Your $\\hat{y}$ in this case are your predicted values from your neural network for each of the $x$ that you generated using `np.linspace`. Make sure to take into account the final activation function!\n",
    "* Your $y$ values are the actual observed values of $f(x)=0.2+0.4x^2+0.3x\\sin(15x)+0.05\\cos(50x)$ for each of the $x$ that you generated using `np.linspace`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 8**: Suppose you wanted to increase the performance of this neural network. How might you go about doing so?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
